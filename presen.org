#+TITLE:  Neuro-Symbolic Classical Planning
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: gif file:img/%s.gif
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+html_head: <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:500,900">
#+html_head_extra:
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center

„ÄÄ

#+begin_larger
Masataro Asai

(MIT-IBM Watson AI Lab, IBM Research, Cambridge)
#+end_larger

„ÄÄ

[[png:MIT-IBM]]

# [[spng:ibm-research]]

#+end_center

#+begin_center
Ë≥™Âïè„ÅØZoom„Å´Êõ∏„ÅçËæº„Åø
#+end_center

#+end_outline-text-1

* You found yourself on Mars, unexpected emergency...

[[png:mars/mars]]

* Two approaches to intelligence : Which is better?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3

[[png:mars/mars]]

#+end_span3
#+begin_span4
#+begin_center
Do you just follow what you *learned* before?
  
"Don't think, feel"?

[[sgif:yoda]]

/Reflex agent/

#+end_center
#+end_span4
#+begin_span5
+
  #+begin_center
  Or do you *think*, adjust, create and choose an appropriate behavior?
  
  [[sgif:mars/yoda-hmph]]

  _/Deliberative agent/_
  #+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_center
+ ‚Üí _I wish to build a *logical, /deliberative/ agent*_

  that operates on the _raw input (e.g., visual input)_
#+end_center

* Classical Planners *solve 8-puzzles << .1sec.*

Symbolic systems excel at deliberation.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span4
[[png:8puzzle-standard]]
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
#+end_span4
#+begin_span2
[[sgif:8puzzle]]
#+end_span2
#+end_row-fluid
#+end_container-fluid

/but only when we have a PDDL model./

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
#+begin_src scheme
(:action       move-up ...
 :precondition (and      (empty ?x ?y-old) ...)
 :effects      (and (not (empty ?x ?y-old))...))
#+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

PDDL : Planning Domain Description Language

* They cannot solve an */Image-based/* 8-puzzle

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+ 
   #+begin_center
   *BECAUSE*

   *WE*

   *DO*

   *NOT*

   *HAVE*

   *A*

   #+begin_larger
   */PDDL/*

   */MODEL/!*
   #+end_larger
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

+ *Latplan (AAAI18, ICAPS19, IJCAI20, JAIR22)* addresses this problem:

  */No Prior Explicit Knowledge/* (i.e. No Human Annotation)

  „ÄÄ */No labels/symbols given/* : e.g. "9 tiles", "moving"

* Key problem Latplan solves: @@html:<br>@@ */Knowledge-Acquisition Bottleneck/*

# * We must *automate 2 processes*:

# * Knowledge-Acquisition Bottleneck:
# 
# #+begin_quote
# The *cost of human* involved for converting *real-world problems* into the inputs for
# domain-independent *symbolic* systems
# #+end_quote



#+begin_container-fluid
#+begin_row-fluid
#+begin_span4

#+begin_alignright
*Visual observations*
#+end_alignright

[[png:overview/1]]

#+end_span4
#+begin_span1
„ÄÄ

„ÄÄ

„ÄÄ

‚Üí

„ÄÄ
#+end_span1
#+begin_span7
#+begin_center
*PDDL Model*
#+end_center
#+begin_src scheme
(:action       move-up
 :precondition
 (and      (empty ?x ?y-old) ...)
 :effects
 (and (not (empty ?x ?y-old))...))
#+end_src

#+end_span7
#+end_row-fluid

#+begin_row-fluid
#+begin_span12
+ We must *automate 2 processes*:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. Symbol Grounding:*

  #+begin_center
  #+begin_larger
  */Symbols/ = words in PDDL*
  #+end_larger
  #+end_center
  
  #+begin_smaller
  | Types        | Examples                     |
  |--------------+------------------------------|
  | Objects      | *panel_7*, *x_0*, *y_0* ...  |
  | Predicates   | (*empty* ?x ?y)              |
  | Propositions | *p_28* = (empty x_0 y_0)     |
  | Actions      | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. Action Model Acquisition:*
  
  #+begin_center
  #+begin_larger
  */Describe/ the*

  */transition rules/*

  *with symbols.*
  #+end_larger
  #+end_center
  
  „ÄÄ

  #+begin_center
  *When* /Empty(x, y_{old}) ‚àß .../ ;

  *Then* /¬¨Empty(x,y_{old}) ‚àß/ ...
  #+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latplan system generates a PDDL (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/2]]









** Latplan system generates a PDDL (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/1]]









** Latplan system generates a PDDL (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/0]]

#+begin_center
All symbols are anonymous & generated (=gensym=)

Note: I am not interested in *human symbols* at all

Requiring human symbols as input = parasitic (*)

+ (How many colors are in a üåà ?)
#+end_center

#+begin_note
(*) Taddeo, Mariarosaria, and Luciano Floridi. "Solving the symbol grounding problem: a critical review of fifteen years of research." Journal of Experimental & Theoretical Artificial Intelligence 17.4 (2005): 419-445.
#+end_note

* Experimental Results

[[spng:experiment/init-goal]]

8-puzzle, 15-puzzle, Lights-Out, Twisted Lights-out, *Blocksworld*, Sokoban

** Success ratio (Arxiv 2021) : 40 problems / domain

[[png:success]]

** Success / Failure cases

[[spng:experiment/blocks-example]]

[[spng:experiment/other-examples]]

* How? */Embedding the formalism/* in the model

+ *Latplan* learns *PDDL-compatible effects*.
+ What is *PDDL compatibility* ?

# *Background Terminology:*
# 
# $z_{t} \rightarrow z_{t+1}$ is called a /progression/
# #+begin_alignright
# ($z_{t+1} \rightarrow z_{t}$ is called a /regression/)
# #+end_alignright

+ Action: $\text{Eat}(\text{pizzaüçï})$ 
+ Precondition: $\lnot \text{full}() \land \text{have}(\text{pizzaüçï})$
+ Effects: *Delete effects:* $\lnot\text{have}(\text{pizzaüçï})$ : -- in the stomach now
+ Effects: *Add effects:* $\text{full}()$

# + Action /eat(person, pizza)/
# + Precondition: ¬¨full(person) ‚àß have(person, pizza)
# + Delete effect: ¬¨have(person, pizza)
# + Add effect: full(person)

# +
#   #+begin_src scheme
#   (:action eat :parameters (?person ?pizza)
#   #+end_src
# +
#   #+begin_src scheme
#   „ÄÄ :precondition (and (not (full ?person))
#   „ÄÄ                   (have ?person ?pizza))
#   #+end_src
# +
#   #+begin_src scheme
#   „ÄÄ :effects (and (not (have ?person ?pizza)) ; delete effects
#   „ÄÄ               (full ?person)))           ; add effects
#   #+end_src

+
  #+begin_quote
  _/Action application in PDDL:/_ $z_{t+1} = (z_t \setminus \text{del}(a)\ )\ \cup\ \text{add}(a)$
  #+end_quote

  Effects $\text{del}(a), \text{add}(a)$ *do not depend on $z_t$*.

+
  #+begin_quote
  */Naive "world model":/* $z_{t+1} = \text{NN}(z_t, a)$ : *No separation*
  #+end_quote

+

  #+begin_larger
  #+begin_center
  Not PDDL compatible ‚Üí */Can't use a fast solver/*
  #+end_center
  #+end_larger

# +
#   #+begin_quote
#   _/PDDL effect:/_ $z_{t+1} = (z_t \ \setminus \ \text{del}(a)\ )\ \cup\ \text{add}(a)$
#   
#   */AAE's effect:/* $z_{t+1} = \text{apply}(z_t, a)$
#   #+end_quote
# 
#   *In PDDL, effects should be consistent for each action*.
# 
#   #+begin_alignright
#   *= Effects should be independent from $z_t$*
#   #+end_alignright

* To use a fast planner we need PDDL @@html:<br>@@ ‚Üí We should */embed PDDL grammar in NN model/*

*In PDDL, effects must not depend on $z_t$*.

#+begin_center
+ ‚Üí A */random variable/* *e* independent from $z_t$.

+ „ÄÄ

  [[png:space-ae/graphical-model-csae]]
#+end_center

# #+begin_smaller
# ÁÑ°Âêç„Ç∑„É≥„Éú„É´„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„ÄÅ$z_t$ „ÇÑ $a$ „Å´„ÅØ *Èõ¢Êï£Èö†„ÇåÂ§âÊï∞* „Çí‰Ωø„ÅÜ (Gumbel-Softmax)
# #+end_smaller

+ Train *discrete latent variables* (how?) via *Variational Inference*

  The graphical model provides a mathematical foundation

* Discrete representation learning: Gumbel-Softmax VAE, etc.

[[png:sae/state-ae-before]]

** Discrete representation learning: Gumbel-Softmax VAE, etc.

[[png:sae/state-ae]]

+ Various methods became available after 2017

  (REBAR, RELAX, VQVAE, SQVAE, ...)


* Important aspects of symbol grounding:                           :noexport:

Definition of symbol: *not* just state variables.

*Symbol* : A structure containing a pointer to a /name/ and a pointer to an /object/.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_src lisp
(defstruct symbol
  name
  object)
#+end_src
#+end_span4
#+begin_span4
#+begin_src python
class Symbol:
  name: string,
  obj: Any
#+end_src
#+end_span4
#+begin_span4
#+begin_src C
struct symbol {
    void* name;
    void* object;
}
#+end_src
#+end_span4
#+end_row-fluid
#+end_container-fluid

A symbol may point to *various things* through a hash table.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_src lisp
(symbol-value    'my-variable)
(symbol-function 'my-function)
(make-instance   'my-class)
#+end_src
#+end_span6
#+begin_span6
#+begin_src python
function_dictionary["myfunc"]
function_dictionary["myfunc"]
#+end_src
#+end_span6
#+end_row-fluid
#+end_container-fluid

* There is no /the symbol/

*/Propositional symbol/* (handempty, ...) vs. _/Action symbol/_ (pickup,...).

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:space-ae/graphical-model-csae-only]]
#+end_span4
#+begin_span8
#+begin_center
+
  #+begin_larger
  They are in a *separate namespace*.
  #+end_larger

  „ÄÄ 

  Same in human symbols:

  „ÄÄ I */like/* an apple : verb

  „ÄÄ Thanks for the */like/* on Facebook! : noun

  Common Lisp (Lisp-2) vs. Scheme (Lisp-1)
#+end_center

#+begin_larger
#+begin_alignright
+ ‚Üí symbols in *different namespace* requires

  *a different constraints.*
#+end_alignright
#+end_larger
#+end_span8
#+end_row-fluid
#+end_container-fluid

* PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/1]]









** PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/2]]









** PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/3]]









** PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/4]]

*Problem symbol:* ‚Üí Different search space

„ÄÄStage 1-1, Stage 1-2, ... in Super Mario Bros.





** PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/5]]

*Problem symbol:* ‚Üí Different search space

„ÄÄStage 1-1, Stage 1-2, ... in Super Mario Bros.

*Domain symbol:*  ‚Üí Different set of available actions

„ÄÄSuper Mario Bros., Donkey Kong, ...

** PDDL: 6 namespaces, 6 mechanisms

[[png:namespace/6]]

*Problem symbol:* ‚Üí Different search space

„ÄÄStage 1-1, Stage 1-2, ... in Super Mario Bros.

*Domain symbol:*  ‚Üí Different set of available actions

„ÄÄSuper Mario Bros., Donkey Kong, ...


* Latplan extension with (ground) First-order logic (ICLR2021 workshop)

*Latplan/FOSAE++* : Learns actions generalized over objects

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:graphical-model-fosae++-nopatch2]]
#+end_span4
#+begin_span8
[[png:blocks-wide]]
#+end_span8
#+end_row-fluid
#+end_container-fluid


+ *Constraint:* Successor State Axiom [Reiter, 1991]
  + *disallow free variables*
+ Action: $a=\text{Eat}(\text{pizza}üçï)$ :
  + *Effect:* */explode(entire room)/* (Prohibited under SSA)
    + A prior that *limits what can happen*
+ Now encoded in NN
  
* Extending the generative model (unpublished)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
+ Lifted actions,

  Parameters *x*
  
  [[png:space-ae/graphical-model-fosae]]

  Predicates, too.

  *Compatible with First Order Logic.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Temporal actions,

  Action durations *d*

  [[png:space-ae/graphical-model-time]]

  *Compatible with LTL.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Domain symbol *D*,

  Problem symbol *P*

  [[png:space-ae/graphical-model-domain]]

  Allows mixed-domain training?
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

# + Discrete-Continuous
#
#  [[png:space-ae/graphical-model-style]]



* Part 1: Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ *Goal: logical, rational agents*

  operating under *raw observations*
  
  [[sgif:mars/yoda-hmph]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ *Latplan*

  [[sjpg:puzzle]]
#+end_center
#+end_span5
#+end_row-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ Each type of symbol requires

  *its own mechanism to learn*
  
  [[png:namespace/6]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ Generative models offer

  the *mathematical rigor*

  [[png:space-ae/graphical-model-csae]]
#+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

# #+begin_center
# Masataro Asai, Hiroshi Kajino, Alex Fukunaga, Christian Muise:
#
# *Classical Planning in Deep Latent Space.* Arxiv 2107.00110 -> JAIR
# #+end_center

+ Ë®òÂè∑Êé•Âú∞„Åô„Çã„Å™„Çâ *ÂØæË±°Ë®ÄË™û„Çí /„Å°„ÇÉ„Çì„Å®/ ÁêÜËß£„Åó„Å¶ /Ê≠£„Åó„Åè/ „É¢„Éá„É´Âåñ„Åó„Çà„ÅÜ!*


* Why _/action symbol grounding/_ is necessary?                    :noexport:

"designers always know the action space"? */Wrong!/*

Humans have a good understanding of low-level actuations...
  
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+
  #+begin_center
  Running
  #+end_center
  [[sjpg:triathlon/running]]
# sjpg:triathlon/bicycle
# sjpg:triathlon/swimming
#+end_span6
#+begin_span6
+ 
  #+begin_center
  Career Path
  #+end_center
  [[sjpg:triathlon/career]]

  #+begin_center
  Do you fully understand all of your opportunities?
  #+end_center
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
+ *Low-level Control*
#+end_center
#+end_span6
#+begin_span6
#+begin_center
+ */High-level Action/*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid



* Part 2:

#+begin_xlarge
Rational Real-World Agent:
#+begin_center
*/the Architecture/*
#+end_center
#+begin_alignright
#+end_alignright
#+end_xlarge

#+begin_alignright
+ ‚Üí Planning + Learning that is not *Representation Learning*
#+end_alignright


* Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/6]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/5]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/4]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/3]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/2]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/1]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** Rational Real-World Agent: the Architecture

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0a]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* Learning forward search heuristics:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_xlarge
#+begin_center
RL

(ICAPS 2022)
#+end_center
#+end_xlarge
#+end_span6
#+begin_span6
#+begin_xlarge
#+begin_center
SL

(IJCAI 2024)
#+end_center
#+end_xlarge
#+end_span6
#+end_row-fluid
#+end_container-fluid

* ICAPS22

[[png:pddlrl/title]]

* Value function learning

SGD Loss function:

\[
\frac{1}{|B|}\sum_{s\in B} \frac{1}{2}\left| V(s) - \mathbb{E}_{a\in A} [Q(a, s)] \right|^2
\]

$B$: batches in the experience replay

#+begin_larger
Research Questions:
#+begin_center
+ *Q: Can RL solve classical planning by itself?* (A: *No*)

+ *Q: Can RL accelerate planning algorithm by itself?* (A: *No*)

+ *Q: With the help of heuristic functions in classical planning?* */(A: Yes)/*
#+end_center
#+begin_alignright
+ Lessons learned: *Don't underestimate the power of planning heuristics...*
#+end_alignright
#+end_larger


* Why Classical Plannning is difficult for RL?

Rewards in Classical Planning:

#+begin_center
#+begin_larger
+ $r(s,a,s') = \left\{\begin{array}{ll} 1 & \text{if}\ s' \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  *This is extremely sparse.*

#+end_larger
#+end_center

+ */Any non-goal states/* are *equally /worthless/*

  + *No guidance* for RL until a goal,

  + which is *difficult to find* in the first place!

* Potential-Based */Reward Shaping/* (PBRS) @@html:<br>@@ for Sparse Rewards

Given a *potential function* $\phi(s)$, */redefine rewards/* as:

\begin{align}
\hat{r}(s, a, s') &= r(s, a, s') + \gamma \phi(s') - \phi(s). \label{eq:shaping1}
\end{align}

+ */Important:/* PBRS *preserves* the optimal value function

  \begin{align}
  V^*_\gamma(s) &= \hat{V}^*_\gamma(s) + \phi(s). \label{eq:shaping2}
  \end{align}

  #+begin_alignright
  #+begin_larger
  = *learning the residual from $\phi(s)$*
  #+end_larger
  #+end_alignright

#+begin_note
Ng, Harada, Russell. "Policy invariance under reward transformations: Theory and application to reward shaping." ICML. Vol. 99. 1999.
#+end_note

* Heuristic Function $h(s)$ in classical planning

+ *Distance estimate*
  + $h(s)=5 \quad \longleftrightarrow \quad s - s_1 - s_2 - s_3 - s_4 - \text{goal}$ („Åä„Åä„Çà„Åù)
  + Many implementations: $h^{\text{add}}$, $h^{\text{FF}}$, $h^{\text{CEA}}$, ...

    #+begin_smaller
    (Bonet&Geffner 01, Hoffmann 01, Helmert 08) „ÇÇ„Å£„Å®Êñ∞„Åó„ÅÑ„ÅÆ„ÇÇ„ÅÇ„Çã
    #+end_smaller

#+begin_xlarge
+ *Heuristics satisfies the PBRS requirements*

+
  #+begin_alignright
  */Let's use $h(s)$ for PBRS!/*
  #+end_alignright
#+end_xlarge

* Issue 1: */Discounting/*

#+begin_larger
*RL : /discounted rewards/* ‚Üî *Planning :* _/undiscounted costs/_
#+end_larger



Our solution:

+ *Training* : Convert $h(s)\rightarrow h_{\gamma}(s)$ : */discounting/* $h(s)$ for PBRS

+
  \begin{align}
  \phi(s) &= h_\gamma(s) = \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.
  \end{align}

  # V_\gamma(s) &= \hat{V}_\gamma(s) + \phi(s) \quad \text{where}\\
  # \phi(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot (-1) = -\frac{1-\gamma^{h(s)}}{1-\gamma}
  # h(s) &= \sum_{t=1}^{h(s)} 1, & h_\gamma(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.


+ *Planning* : Convert $V_{\gamma}(s)\rightarrow V(s)$ : _/undiscounting/_ a learned $V_{\gamma}(s)$

+
  \begin{align}
  V_\gamma(s) = \dfrac{1 - \gamma^{V(s)}}{1 - \gamma}
  &\Leftrightarrow
  V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s)).
  \end{align}

* Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm6]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm5]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm4]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm3]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm2]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm1]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm]]

+ *Size* & *Permutation invariant* to FOL binary inputs

+ *Quite useful for various FOL tasks*
  + ILP [Dong et. al. ICLR 2019]
  + Regression (our work)

* Results

Skipped

* ICAPS 2022 : Conclusion

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *Sparse Rewards*

  #+begin_smaller
  $r(s) = \left\{\begin{array}{ll} 1 & \text{if}\ s \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  „Ç¥„Éº„É´„Å´„Åü„Å©„ÇäÁùÄ„Åè„ÅÆ„Åå

  „Å®„Å¶„ÇÇÈõ£„Åó„ÅÑ
  #+end_smaller
#+end_span6
#+begin_span6
+ *Potential-Based*

  *Reward Shaping*

  $V^*_\gamma(s) = \hat{V}^*_\gamma(s) + \phi(s)$

  #+begin_smaller
  $\phi(s)=$ Ë∑ùÈõ¢„ÅÆË¶ãÁ©ç„ÇÇ„Çä $h(s)$

  Sparse Reward „ÇíËß£Ê±∫
  #+end_smaller
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Discounting*

  $h_\gamma(s)  = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}$

  $V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s))$

  #+begin_smaller
  „Åì„Çå„Åå„Å™„ÅÑ„Å®

  $h(s)=\infty$ „ÅÆ„Å®„ÅçÊ≠ª„Å¨
  #+end_smaller
#+end_span6
#+begin_span6
+ *First-Order Logic Input*

  [[png:pddlrl/nlm]]

  #+begin_smaller
  Áï∞„Å™„Çã„Çµ„Ç§„Ç∫„ÅÆÂïèÈ°å„Å´Ê±éÂåñ
  #+end_smaller

#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_smaller
+ *Q: RL„Å†„Åë„Åß Âè§ÂÖ∏„Éó„É©„É≥„Éã„É≥„Ç∞„ÇíËß£„Åë„Çã„Åã?* (A: *‰∏çÂèØ*)

  *Q: RL„Å†„Åë„Åß „Éó„É©„É≥„Éã„É≥„Ç∞„ÇíÈ´òÈÄüÂåñ„Åß„Åç„Çã„Åã?* (A: *‰∏çÂèØ*)

  *Q: „Éó„É©„É≥„Éã„É≥„Ç∞„ÅÆÈÅìÂÖ∑„Å®ÈÅ©Âàá„Å´ÁµÑ„ÅøÂêà„Çè„Åõ„Çå„Å∞?* */(A: ÂèØ)/*

  Lessons learned: */Âè§ÂÖ∏„Éó„É©„É≥„Éã„É≥„Ç∞„ÇíÁîò„ÅèË¶ã„Çã„Å™/*
#+end_smaller
#+end_center


* IJCAI24

[[png:truncated-gaussian/title]]



#+begin_xlarge
#+begin_center
RL Â´å„ÅÑ„Å™„Çì„Åß ... ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí!
#+end_center
#+end_xlarge

* Task: Learn the shortest path cost

[[png:truncated-gaussian/1b]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1a]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1]]

#+begin_larger
#+begin_center
+ *Q: Can we exploit the /admissibility/ of heuristics* in training?
#+end_center
#+end_larger

* Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2j]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2i]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2h]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2g]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2f]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2e]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2d]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2c]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2b]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2a]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2]]

*

[[png:truncated-gaussian/3c]]

**

[[png:truncated-gaussian/3b]]

**

[[png:truncated-gaussian/3a]]

**

[[png:truncated-gaussian/3]]

* Thinking with Distributions                                      :noexport:

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

+ We know $h^*\ge 0$
+ We know $h^*\ge h$ : *admissible heuristics*
+ *Why the heck* do we assign */non-zero probability/* to $h^*< 0$?
+ i.e. $\mathcal{N}(\mu,\sigma)$ *ignores* our */expert knowledge/* on $h^*$
+ It is not the *correct* distribution

* Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4e]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4d]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4c]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4b]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4a]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

* What is the correct distribution?

+ Follow the */Maximum Entropy Principle/* (Jaynes 1957):

  #+begin_quote
  + /Choose the distribution with the *largest entropy*/
  + /among those that satisfy the *expert knowledge / constraint*./
  #+end_quote

+
  #+begin_alignright
  #+begin_larger
  ‚Ü™ We know a lower bound $h\le h^*$ !

  *This is our expert knowledge!*
  #+end_larger
  #+end_alignright

* What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5c]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5b]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5a]]

** Results

[[png:truncated-gaussian/6]]

* IJCAI 2024 : Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *We learn* $h^*$

  [[png:truncated-gaussian/1]]

#+end_span6
#+begin_span6
+ *Square error = Gaussian*

  [[png:truncated-gaussian/3]]
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Gaussian is a lie*

  [[png:truncated-gaussian/4]]
#+end_span6
#+begin_span6
+ *Max-ent: Truncated Gaussian*

  [[png:truncated-gaussian/5a]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

Take-home message: *Whenever you see a square error, doubt it!*

* ÁêÜÊÄßÁöÑ„Å™ÂÆü‰∏ñÁïå„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0b]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* Êé¢Á¥¢„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÊîπÂñÑ:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_xlarge
#+begin_center
MCTS + Gaussian Bandit

(ECAI 2024)
#+end_center
#+end_xlarge
#+end_span6
#+begin_span6
#+begin_xlarge
#+begin_center
MCTS + Power Bandit

(SoCS 2024 extended abstract)
#+end_center
#+end_xlarge
#+end_span6
#+end_row-fluid
#+end_container-fluid

* ECAI 2024

[[png:scale-invariant/title]]

* How Traditional Algorithms work : Dijkstra (1956)                :noexport:

dfs bfs *dijkstra* A* GBFS

[[png:search/astar2]]

Dijkstra: *priority queue sorted by* $g(n)$


** How Traditional Algorithms work : Dijkstra (1956)

dfs bfs *dijkstra* A* GBFS

[[png:search/astar3]]

Dijkstra: *priority queue sorted by* $g(n)$


** How Traditional Algorithms work : Dijkstra (1956)

dfs bfs *dijkstra* A* GBFS

[[png:search/astar4]]

Dijkstra: *priority queue sorted by* $g(n)$



** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar5]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar6]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar7]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs0]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs1]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs2]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$



** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs3]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/astar7]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$


** How Traditional Algorithms work : */Hill Climbing/* in RL

dfs bfs dijkstra A* *GBFS*

[[png:search/hc1]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Hill Climbing/* in RL

dfs bfs dijkstra A* *GBFS*

[[png:search/hc2]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

*Importance of Priority Queue!!!*

* Monte Carlo Tree Search (General version)

[[png:search/mcts01]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts02]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts03]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts04]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts05]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts06]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts07]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ |
|-------------+----------------------+----------------------|
| Game        | ‚úó Hand-coded         |                      |
| (e.g., Go)  | ‚úó Not accurate       |                      |
|-------------+----------------------+----------------------|
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | „ÄÄ                    | „ÄÄ                    |
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | „ÄÄ                    | „ÄÄ                    |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|-------------+----------------------+------------------------|
| Game        | ‚úó Hand-coded         | ‚úî Finite horizon (9x9) |
| (e.g., Go)  | ‚úó Not accurate       |                        |
|-------------+----------------------+------------------------|
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | „ÄÄ                    | „ÄÄ                      |
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | „ÄÄ                    | „ÄÄ                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|------------+--------------------+----------------------|
| Game       | ‚úó Hand-coded       | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate     | ‚úî Dense reward       |
|------------+--------------------+----------------------|
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ     | „ÄÄ                  | „ÄÄ                    |
| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ     | „ÄÄ                  | „ÄÄ                    |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | ‚úó Hand-coded         | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate       | ‚úî Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      |                      |
| Planning   |                      |                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | ‚úó Hand-coded         | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate       | ‚úî Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      | ‚úó Infinite horizon   |
| Planning   |                      |                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | ‚úó Hand-coded         | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate       | ‚úî Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      | ‚úó Infinite horizon   |
| Planning   |                      | ‚úó Sparse reward      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | Heuristic function „ÄÄ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | ‚úó Hand-coded         | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate       | ‚úî Dense reward       |
|------------+----------------------+----------------------|
| Classical  | ‚úî Automated          | ‚úó Infinite horizon   |
| Planning   |                      | ‚úó Sparse reward      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ | */Heuristic function/* „ÄÄ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | ‚úó Hand-coded         | ‚úî Finite horizon (9x9)     |
| (e.g., Go) | ‚úó Not accurate       | ‚úî Dense reward       |
|------------+----------------------+----------------------|
| Classical  | ‚úî Automated          | ‚úó Infinite horizon   |
| Planning   | ‚úî Domain-Independent | ‚úó Sparse reward      |

* Bandit for Action Selection                                      :noexport:

[[png:bandit/1]]

** Bandit for Action Selection

[[png:bandit/2]]

** Bandit for Action Selection

[[png:bandit/3]]

* Backup in MCTS : Avg

[[png:tree-based/1]]

** Backup in MCTS : Avg vs. Min

[[png:tree-based/2]]


** Backup in MCTS : Avg vs. Min = */GBFS, A*/*

[[png:tree-based/3]]

#+begin_center
#+begin_xlarge
+ *GBFS: Traditional SotA*
+ UCT has been _/bad/_

  (UCT = MCTS+UCB1)
#+end_xlarge
#+end_center

#+begin_note
Schulte and Keller. "Balancing exploration and exploitation in classical planning." SOCS, 2014
#+end_note

** GBFS */can/* be seen as MCTS (Keller et al. 2013)               :noexport:

|   |           |                |                                       <r> |
|   | Algorithm |                |                          Action Selection |
|---+-----------+----------------+-------------------------------------------|
|   | */GBFS/*  | */MCTS + Min/* |          $\hspace{3em}h(s)\hspace{6.5em}$ |
|   | GreedyUCT | MCTS + Avg     | $\hspace{3em}h(s)-c\sqrt{{\log N}/{n_i}}$ |

#+begin_xlarge
#+begin_center
+ MCTS+UCB1 < */MCTS+Min/*
#+end_center
#+end_xlarge

* Search / Planning community is SO bad at statistics (more wrong than we initially thought) :noexport:

*Nobody* understands the original UCB paper [Auer 2002] properly. üò¢

+ [Keller 2013] uses UCB1 in [Auer 2002], which assumes [0, 1] rewards
  + *BUT!* Node evaluation is a distance to the goal, which is *unbounded*
  + To satisfy [0,1], normalize $h(s_i)$ by min, max over arms (hack! üò†)
+ [Auer 2002] *already* proposed *UCB1-Normal* (Theorem 4)
  + UCB1 for the unbounded rewards == Gaussian distributed rewards
+ [Tesauro 2010] *rediscovers* UCB1-Normal, *but does not realize it! ü§Ø*
  + UCB1 was widely known in 2010, but no one knew UCB1-Normal ü§î

#+begin_note
Keller, T.; and Helmert, M. 2013. Trial-Based Heuristic Tree Search for Finite Horizon MDPs. ICAPS

Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-Time Analysis of the Multiarmed Bandit Problem. Machine Learning

Tesauro, G.; Rajan, V.; and Segal, R. 2010. Bayesian Inference in Monte-Carlo Tree Search. UAI
#+end_note

* Why: */Poor theoretical understanding/* of bandits.

#+begin_quote
$\text{UCB1}_i = \mu_i-c\sqrt{\frac{\log N}{n_i}}$ , *for* $\mu_i \in [0,c]$

#+begin_center
$N$: parent visit, $n_i$: visit for child $i$

$\mu_i$ : mean over the subtree of $i$
#+end_center
#+end_quote

+ *Rewards must have a /known, fixed, shared reward range/*.

  + But $h(s)$ *have /no known range!/*

    _Requirments unsatisfied. All theoretical guarantees violated._

+ *Our solution: Gaussian Bandit* defined on *Entire* $\mathbb{R}=[-\infty,\infty]$

  #+begin_quote
  $\text{UCB1-Normal2}_i = \mu_i-\sigma_i \sqrt{\log N}$ , *for* $\mu_i \in \mathbb{R}$

  #+begin_center
  $\mu_i, \sigma_i$ : mean / stdev over the subtree of $i$
  #+end_center
  #+end_quote

* Results (ECAI 2024)

[[spng:ecai24-results]]

* Avg vs. Min                                                      :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/5]]
#+end_span6
#+begin_span6
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
#+end_span6
#+begin_span6
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
+ *Least average* :: *on average* close to the goal
  + Bad nodes can *hide* goals!
#+end_span6
#+begin_span6
[[png:tree-based/6]]
+ *Least minima (preferred)* :: *at best* close to the goal
  + Bad nodes do not matter!
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min               :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
#+end_span6
#+begin_span6
Select a subtree by...

+ *Least average* :: *on average* close to the goal
  + Bad nodes can *hide* goals!
+ *Least minima (preferred)* :: *in the best case* close to the goal
  + Bad nodes do not matter!
#+end_span6
#+end_row-fluid
#+end_container-fluid


Characterized by the *backup strategy difference*.

* Avg. vs. Min. = Different Backup strategy                        :noexport:

+ Standard UCB1-Normal --- avg ::
  Backup $(\sum_i n_i,\ \frac{\sum_i n_i \mu_i}{\sum_i n_i})$ : weighted sum of means
+ Keller et al. 2013 --- */minimum of means/* without statistical justification ::
  Backup $(\sum_i n_i,\ \min_i \mu_i)$
+ Tesauro et al. 2010 --- */approximate minimum/* ::
  Backup a dist. of minimum of Gaussians [Clerk, 1961]
  # + If $X_1\sim \mathcal{N}(\mu_1,\sigma_1), X_2\sim \mathcal{N}(\mu_2,\sigma_2)$, then
  # + $\max(X_1,X_2)\sim \mathcal{N}(\mu_3,\sigma_3)$ where
  # + $\mu_3=\mu_1\Phi(\alpha)+\mu_2\Phi(-\alpha)+(\sigma_1^2+\sigma_2^2)\phi(\alpha)$
  # + $\sigma_3=(\mu_1^2+\sigma_1^2)\Phi(\alpha)+(\mu_2^2+\sigma_2^2)\Phi(-\alpha)+(\mu_1+\mu_2)(\sigma_1^2+\sigma_2^2)\phi(\alpha)$
  # + *Cons*: Not exact for $>2$ arms, *Pros*: Statistical Justification

#+begin_quote
#+begin_center
#+begin_xlarge
None outperforms GBFS
#+end_xlarge
in our reproduction
#+end_center
#+end_quote

* Experiments                                                      :noexport:

|                                                                              <r> |                                       |                |              |
| /                                                                                | <                                     |                | >            |
| selection \ backup                                                               | $\frac{\sum_i n_i \mu_i}{\sum_i n_i}$ | $\min_i \mu_i$ | Tesauro      |
|----------------------------------------------------------------------------------+---------------------------------------+----------------+--------------|
|                                                                          $\mu_i$ |                                       | GBFS           |              |
| $\frac{\mu_i-\min_j h(s_j)}{\max_j h(s_j)-\min_j h(s_j)}-c\sqrt{{\log N}/{n_i}}$ | Bounded UCB                           | Bounded UCB*   | Bounded UCB+ |
|                                                   $\mu_i-c\sqrt{{\log N}/{n_i}}$ | UCB                                   | UCB*           | UCB+         |
|                                                   $\mu_i-\sigma_i \sqrt{\log N}$ | UCB1-Normal                           | UCB1-Normal*   | UCB1-Normal+ |

There are also Std FF vs Rnd FF versions, $c\in\{0.3,1.0,3.0\}$

Too many, ‚Üí *We focus on UCB1-Normal*

** Results vs. GBFS, deterministic FF : not conclusive

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
*Std FF, mean (win)*

[[spng:plots/expansions__UCT-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
Std FF, min

[[spng:plots/expansions__UCTStar-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Std FF, tesauro (win)*

[[spng:plots/expansions__UCTPlus-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results vs. GBFS with mean of randomized FF

Since GBFS + mean randFF < GBFS + min randFF, this is a straw-man comparison

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT mean (win)*

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT min (win)*

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT tesauro (win)*

[[spng:plots/expansions__UCTPlus-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results vs. GBFS with min of randomized FF

UCB1-Normal, UCB1-Normal*, UCB1-Normal+ are *worse* than GBFS with min of randomized FF!

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT mean (lost)/

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT min (lost)/

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT tesauro (lost)/

[[spng:plots/expansions__UCTPlus-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results: Tesauro vs mean, min (not conclusive)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
Std FF, vs mean

[[spng:plots/expansions__UCT-False-10-0.2-inf-0.5-False__UCTPlus-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+begin_span6
#+begin_center
*Std FF, vs min (win)*

[[spng:plots/expansions__UCTStar-False-10-0.2-inf-0.5-False__UCTPlus-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
/Rnd FF, mean (lost)/

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__UCTPlus-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+begin_span6
#+begin_center
Rnd FF, min

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__UCTPlus-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid


** Conclusion of reproducing existing work

* New Algorithm : Model the minimum */correctly/*

Fit the data to...

|             |                                         |        <c>         |
| UCB1        | *a /known/ finite support distribution* | $[0,1]$ , $[3,5]$  |
| UCB1-Normal | *a Gaussian distribution*               | $[-\infty,\infty]$ |

#+begin_quote
+ *Do heuristics have /known/ bounds e.g. $[3,5]$ ? /No!/*

+ *Are heuristic samples $\in[-\infty,\infty]$ ? /No! It is non-negative!/*

  $0 \ge h(s)$

#+begin_larger
#+begin_center
+ *Gaussian assumption is /STILL WRONG/*
#+end_center
#+end_larger

#+end_quote

# + *Fact* : Heuristic $h^{\text{FF}}$ has */unknown/* bounds
#
#   \[
#   0 \leq h^+(s) \leq h^{\text{FF}}(s) < \ ?
#   \]
#
#   (NP-hard to compute $h^+$ == unknown, $?$ is also unknown)

* What distribution is the right answer?

|             | Range            | How it is distributed?          |
|-------------+------------------+---------------------------------|
| Gaussian    | _/Unbounded/_ ‚úó  |                                 |
| Exponential | */Half-Bounded/* |                                 |
| Uniform     | */Bounded/*      |                                 |
|             |                  |                                 |
|             |                  | „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ |


[[png:scale-invariant/candidate-distribution]]

** What distribution is the right answer?

|             | Range            | How it is distributed?              |
|-------------+------------------+-------------------------------------|
| Gaussian    | _/Unbounded/_ ‚úó  |                                     |
| Exponential | */Half-Bounded/* | _/Smallest value is most common/_ ‚úó |
| Uniform     | */Bounded/*      | Equally likely around min and max   |
|             |                  |                                     |
|             |                  | „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ     |

[[png:scale-invariant/candidate-distribution]]

** What distribution is the right answer?

|             | Range            | How it is distributed?              |
|-------------+------------------+-------------------------------------|
| Gaussian    | _/Unbounded/_ ‚úó  |                                     |
| Exponential | */Half-Bounded/* | _/Smallest value is most common/_ ‚úó |
| Uniform     | */Bounded/*      | Equally likely around min and max   |
| *Power*     | */Bounded/*      | */Smallest value is rare/*          |
|             |                  | „ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ„ÄÄ     |

[[png:scale-invariant/power]]

** What distribution is the right answer?                          :noexport:

*Power* distribution : *Bounded* distribution, with *largest point = /rarest/*

Sign flip ($\min \leftrightarrow \max$) will achieve the desired effect

[[spng:gpd]]

* But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

** But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

*Extreme Value Theorem type 1*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
 M_n= \max     \frac{x_i-b_n}{a_n}  \longrightarrow \mathrm{EVD}(\mu, \sigma, \xi)
\]

** But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

*Extreme Value Theorem type 1*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
 M_n= \max     \frac{x_i-b_n}{a_n}  \longrightarrow \mathrm{EVD}(\mu, \sigma, \xi)
\]

*/Extreme Value Theorem type 2/*: Given i.i.d. RVs $x_0 < \ldots < x_k < \ldots < x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
  p(x | x > x_k)  \longrightarrow \mathrm{GPD}(\mu, \sigma, \xi)
\]

$\mathrm{GPD}$ : */Generalized Pareto Distribution/* is the way to go. *Why?*

** Power, Uniform, Exponential, Pareto are special cases of Generalized Pareto $\mathrm{GPD}(\theta, \sigma, \xi)$.

|                           |   <c>    |             <c>             |
|                           |   min    |             max             |
|---------------------------+----------+-----------------------------|
| $\xi>0$ = Pareto          | $\theta$ |          $\infty$           |
| $\xi=0$ = Exponential     | $\theta$ |          $\infty$           |
| $\xi<0$ = (Flipped) Power | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |
| $\xi=-1$ = Uniform        | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |
| $\xi<-1$                  | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |

[[spng:gpd]]

* Modeling the average

[[png:clt-vs-evt/0]]

** Modeling the average = Fitting the dist. with Gaussian

[[png:clt-vs-evt/1]]

** Modeling the maximum

[[png:clt-vs-evt/2]]

** Modeling the maximum = Fitting the tail dist. with GP

[[png:clt-vs-evt/3]]

* New Algorithm : Model the minimum */correctly/*

Fitting GPD itself is *difficult* (computational issue)

\rightarrow fit subclasses: *Uniform* and *Power*

|                | Lower bound        | Upper bound     | Shape               |
|----------------+--------------------+-----------------+---------------------|
| UCB1           | _/Known/_ $0$      | _/Known/_ $1$   |                     |
| UCB1-Normal    | $-\infty$          | $\infty$        |                     |
| *UCB1-Uniform* | */Unknown/* $\ell$ | */Unknown/* $u$ | _/Fixed, Flat/_     |
| *UCB1-Power*   | _/Known/_ $0$      | */Unknown/* $u$ | */Rare near goals/* |


$\text{UCB1-Uniform}_i = \frac{u_i+l_i}{2} -(u_i-l_i)\sqrt{6n_i \log N}$

$\text{UCB1-Power}_i = \frac{u_ia_i}{a_i+1} -u_i\sqrt{6n_i \log N}$


* Results (SoCS 2024 Extended Abstract)

[[spng:extreme-results]]

* ECAI 2024 + SoCS 2024: Conclusion

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
+ *UCT is slow in*

  Classical Planning

  $\mu - c \sqrt{ \frac{\log N}{n_i} }$

+ *Why?*

  $h(s) \not\in [0,c]$

  (fixed range)

#+end_span4
#+begin_span4
+ (ECAI 2024)

  *Use Gaussian Bandit!*

  $\mu - \sigma \sqrt{ \log N }$



  $h(s) \in [-\infty,\infty]$



+ */STILL WRONG!/*

  $0 \le h(s)$
#+end_span4
#+begin_span4
+ (SoCS 2024)

  *Use Uniform / Power!*

  $\frac{u_i+l_i}{2} -(u_i-l_i)\sqrt{6n_i \log N}$

  $\frac{u_ia_i}{a_i+1} -u_i\sqrt{6n_i \log N}$

#+end_span4
#+end_row-fluid
#+end_container-fluid

#+begin_xlarge
+ Use bandits correctly!
#+end_xlarge

#+end_center



* Conclusion & Take-home message

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
+ Ë®òÂè∑Êé•Âú∞:
  + PDDL„Çí *Ê≠£„Åó„Åè* „É¢„Éá„É´Âåñ„Åó„Çà„ÅÜ!
    + ‚Üí„Ç∞„É©„Éï„Ç£„Ç´„É´„É¢„Éá„É´
+ „Éí„É•„Éº„É™„Çπ„ÉÜ„Ç£„ÇØ„ÇπÂ≠¶Áøí:
  + RL : Reward Shaping „Çí *Ê≠£„Åó„Åè* „ÇÑ„Çç„ÅÜ!
    + */Discouning/* , _/Undiscounting/_
  + SL : $h^*(s)$ „Çí *Ê≠£„Åó„Åè* „É¢„Éá„É´Âåñ„Åó„Çà„ÅÜ!
    + Truncated Gaussian
+ Êé¢Á¥¢„Ç¢„É´„Ç¥„É™„Ç∫„É†ÊîπÂñÑ / Bandit:
  + $h(s)$ „Çí *Ê≠£„Åó„Åè* „É¢„Éá„É´Âåñ„Åó„Çà„ÅÜ!
    + ‚Üí Gaussian, Power
#+end_span8
#+begin_span4
[[png:nesyarch/0b]]

#+begin_larger
#+begin_center
+ *Ê≠£„Åó„Åï„Å∏„ÅÆ*

  *„Åì„Å†„Çè„Çä„ÅØ*

  *„ÅÑ„Å§„Åã*

  *ÂÆü„ÇíÁµê„Å∂„ÅØ„Åö*

  (Â§ßÂ§â„Å†„Åë„Å©)

#+end_center
#+end_larger

#+end_span4
#+end_row-fluid
#+end_container-fluid

# + ÊÑèÂë≥„ÅÇ„Çã„Éã„É•„Éº„É≠„Ç∑„É≥„Éú„É™„ÉÉ„ÇØ„Çí„ÇÑ„Çã„Å´„ÅØ
#
#   PhD2ÂõûÂàÜÂãâÂº∑„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä (Ë®òÂè∑Á≥ª+Â≠¶ÁøíÁ≥ª)
#
# + Ë®òÂè∑Á≥ª„ÅÆ‰∫∫„ÅØ„ÄÅÂ≠¶ÁøíÁ≥ª„ÅÆ‰∫∫„Çí„Éê„Ç´„Å´„Åó„Åô„Åé„Å™„ÅÑ„Çà„ÅÜ„Å´
#
# + Â≠¶ÁøíÁ≥ª„ÅÆ‰∫∫„ÅØ„ÄÅË®òÂè∑Á≥ª„ÅÆ‰∫∫/„Çø„Çπ„ÇØ„ÇíË¶ã„Åè„Å≥„Çâ„Å™„ÅÑ„Çà„ÅÜ„Å´

