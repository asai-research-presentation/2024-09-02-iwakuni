#+TITLE:  ãƒ‹ãƒ¥ãƒ¼ãƒ­ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ Classical Planning
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: gif file:img/%s.gif
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+html_head: <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:500,900">
#+html_head_extra:
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center



#+begin_larger
Masataro Asai

(MIT-IBM Watson AI Lab, IBM Research, Cambridge)
#+end_larger



[[png:MIT-IBM]]

# [[spng:ibm-research]]

#+end_center

#+begin_center
è³ªå•ã¯Zoomã«æ›¸ãè¾¼ã¿
#+end_center

#+end_outline-text-1

* Part 1:

#+begin_xlarge
#+begin_center
è¨˜å·æ¥åœ°ã¨

ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«
#+end_center
#+end_xlarge

* ç«æ˜Ÿã«å–ã‚Šæ®‹ã•ã‚ŒãŸ! ã†ã‚ ã‚„ã£ã¹ ã©ã†ã—ã‚ˆã†?

[[png:mars/mars]]

* ã©ã£ã¡ã®ã€ŒçŸ¥æ€§ã€ãŒã„ã„ã§ã™ã‹?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3

[[png:mars/mars]]

#+end_span3
#+begin_span4
#+begin_center
*çµŒé¨“ã«åŸºã¥ãå‹˜ã ã‘* ã§ã©ã†ã«ã‹ãªã‚‹?

"è€ƒãˆã‚‹ãªã€æ„Ÿã˜ã‚" ã§ã†ã¾ãè¡Œãã¾ã™ã‹?

[[sgif:yoda]]

/Reflex (åå°„) agent/

#+end_center
#+end_span4
#+begin_span5
+
  #+begin_center
  ã¡ã‚‡ã£ã¨ *é ­ã‚’ä½¿ã£ã¦* è‡¨æ©Ÿå¿œå¤‰ã«å¯¾å¿œã—ã¾ã™ã‹?

  [[sgif:mars/yoda-hmph]]

  _/Deliberative (ç†Ÿè€ƒ) agent/_
  #+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_center
+ â†’ _ç§ãŒã‚„ã‚ŠãŸã„ã®ã¯ *ç†Ÿè€ƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ*_

  ã—ã‹ã‚‚ã€ _ç”Ÿã®ã‚»ãƒ³ã‚µãƒ¼å…¥åŠ›(ç”»åƒ)ã‚’ç›´æ¥ä½¿ãˆã‚‹ã‚„ã¤_
#+end_center

* å¤å…¸ãƒ—ãƒ©ãƒ³ãƒŠ (å¤å…¸è‡ªå‹•è¨ˆç”»ã‚½ãƒ«ãƒ): @@html:<br>@@ *8ãƒ‘ã‚ºãƒ«ã‚’ãƒŸãƒªç§’å˜ä½ã§è§£ã‘ã‚‹.*

è¨˜å·çš„æ¢ç´¢ãƒ»æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã¯çˆ†é€Ÿ

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span4
[[png:8puzzle-standard]]
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
#+end_span4
#+begin_span2
[[sgif:8puzzle]]
#+end_span2
#+end_row-fluid
#+end_container-fluid

/ã§ã‚‚ PDDL ãŒã‚ã‚‹ã¨ãã«ã—ã‹ é©ç”¨ä¸å¯èƒ½./

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
#+begin_src scheme
(:action       move-up ...
 :precondition (and      (empty ?x ?y-old) ...)
 :effects      (and (not (empty ?x ?y-old))...))
#+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

PDDL : Planning Domain Description Language

* å¼±ç‚¹: */ç”»åƒãƒ™ãƒ¼ã‚¹å…¥åŠ›/* ã«ãªã‚‹ã¨é©ç”¨ä¸å¯èƒ½

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+
   #+begin_center
   *ãªãœãªã‚‰*

   *ãƒ—ãƒ©ãƒ³ãƒŠã¯*

   *å®Ÿè¡Œã™ã‚‹ã®ã«*

   #+begin_larger
   */PDDL/*

   */ãƒ¢ãƒ‡ãƒ«ãŒ/*

   */å¿…è¦/!*
   #+end_larger
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

+ *Latplan (AAAI18, ICAPS19, IJCAI20, JAIR 22)* ã¯ã“ã®å•é¡Œã‚’è§£æ±º

  */ç”»åƒã«å¯¾ã™ã‚‹äº‹å‰çŸ¥è­˜ãªã—/* (ç”»åƒã«å¯¾ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©)

  ã€€ */äººã«ç”±æ¥ã™ã‚‹ãƒ©ãƒ™ãƒ«ãƒ»è¨˜å·ã¯ä¸€åˆ‡ãªã—/* : "ã‚¿ã‚¤ãƒ«ãŒ9ã¤ã‚ã‚‹", "å‹•ã"

* Latplan ã®ç›®æ¨™: */çŸ¥è­˜ç²å¾—ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è§£æ¶ˆ/*

# * We must *automate 2 processes*:

# * Knowledge-Acquisition Bottleneck:
#
# #+begin_quote
# The *cost of human* involved for converting *real-world problems* into the inputs for
# domain-independent *symbolic* systems
# #+end_quote



#+begin_container-fluid
#+begin_row-fluid
#+begin_span4

#+begin_alignright
*çŠ¶æ…‹é·ç§»ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿*
#+end_alignright

[[png:overview/1]]

#+end_span4
#+begin_span1






â†’


#+end_span1
#+begin_span7
#+begin_center
*PDDL Model*
#+end_center
#+begin_src scheme
(:action       move-up
 :precondition
 (and      (empty ?x ?y-old) ...)
 :effects
 (and (not (empty ?x ?y-old))...))
#+end_src

#+end_span7
#+end_row-fluid

#+begin_row-fluid
#+begin_span12
+ Latplan ãŒè‡ªå‹•åŒ–ã™ã¹ãä½œæ¥­ã¯ *2ã¤* ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯åˆ¥ã®ã‚¿ã‚¹ã‚¯:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. è¨˜å·ç”Ÿæˆ:*

  #+begin_center
  #+begin_larger
  */ã‚·ãƒ³ãƒœãƒ«/ = PDDLå†…ã§ä½¿ã‚ã‚Œã‚‹èªå½™*
  #+end_larger
  #+end_center

+
  #+begin_smaller
  | Types        | Examples                     |
  |--------------+------------------------------|
  | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | *panel_7*, *x_0*, *y_0* ...  |
  | è¿°èª | (*empty* ?x ?y)              |
  | å‘½é¡Œ | *p_28* = (empty x_0 y_0)     |
  | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ç²å¾—:*

  #+begin_center
  #+begin_larger
  */çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«ã‚’/*

  */ã‚·ãƒ³ãƒœãƒ«ã‚’ç”¨ã„ã¦/*

  */è¡¨ç¾ã™ã‚‹ã“ã¨/*
  #+end_larger
  #+end_center

+

  #+begin_center
  *When* /Empty(x, y_{old}) âˆ§ .../ ;

  *Then* /Â¬Empty(x,y_{old}) âˆ§/ ...
  #+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/2]]

ã€€

ã€€

ã€€

ã€€

** Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/1]]

ã€€

ã€€

ã€€

ã€€

** Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/0]]

#+begin_center
+ Latplan ãŒç”Ÿæˆãƒ»æ¥åœ°ã™ã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯ ã™ã¹ã¦ *ç„¡åã‚·ãƒ³ãƒœãƒ«* (=gensym=)

  *äººé–“ã®ä½¿ã† è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«* ã¯æ‰±ã‚ãªã„

+ è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«ã¸ã®æ¥åœ°ã¯ *å¯„ç”Ÿçš„* è¨˜å·æ¥åœ°(*) ã¨ã‚ˆã°ã‚Œã‚‹

  *ç†ç”±:* äººé–“ã®ä½œã£ãŸä¸–ç•Œã®é›¢æ•£åŒ–ã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚‰ (ä¾‹: *è™¹ğŸŒˆã¯7è‰²?*)

  #+begin_note
  (*) Taddeo, Mariarosaria, and Luciano Floridi. "Solving the symbol grounding problem: a critical review of fifteen years of research." Journal of Experimental & Theoretical Artificial Intelligence 17.4 (2005): 419-445.
  #+end_note
#+end_center


* Latplan ãŒè§£ã‘ã‚‹ãƒ‘ã‚ºãƒ«/ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å•é¡Œã®ä¾‹

[[spng:experiment/init-goal]]

8-puzzle, 15-puzzle, Lights-Out, Twisted Lights-out, *Blocksworld*, Sokoban

** æˆåŠŸç‡ (1ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚ãŸã‚Š40ã‚¿ã‚¹ã‚¯)

[[png:success]]

** æˆåŠŸä¾‹ / å¤±æ•—ä¾‹

[[spng:experiment/blocks-example]]

# [[spng:experiment/other-examples]]

* ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®è‚: *ç”Ÿæˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°* + @@html:<br>@@ _/ã¡ã‚ƒã‚“ã¨å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®ã“ã¨ã‚’å‹‰å¼·ã™ã‚‹ã“ã¨/_

+ Latplan ã¯ *PDDLã¨äº’æ›æ€§ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ç”»åƒã‹ã‚‰å­¦ç¿’ã™ã‚‹*.
+ *ã€ŒPDDLã¨äº’æ›æ€§ãŒã‚ã‚‹ã€ã£ã¦ã©ã†ã„ã†æ„å‘³ ?*

# *Background Terminology:*
#
# $z_{t} \rightarrow z_{t+1}$ is called a /progression/
# #+begin_alignright
# ($z_{t+1} \rightarrow z_{t}$ is called a /regression/)
# #+end_alignright

+ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: $a=\text{Eat}(\text{pizzağŸ•})$
+ å‰ææ¡ä»¶: $\text{pre}(a)=\lnot \text{full}() \land \text{have}(\text{pizzağŸ•})$ --- ç©ºè…¹ã‹ã¤ãƒ”ã‚¶ãŒã‚ã‚‹
+ åŠ¹æœ: *å‰Šé™¤åŠ¹æœ:* $\text{del}(a)=\{\text{have}(\text{pizzağŸ•}) \}$ --- ãƒ”ã‚¶ã¯ãªããªã£ã¦
+ åŠ¹æœ: *è¿½åŠ åŠ¹æœ:* $\text{add}(a)=\{\text{full}() \}$ --- æº€è…¹ã«ãªã‚‹

# + Action /eat(person, pizza)/
# + Precondition: Â¬full(person) âˆ§ have(person, pizza)
# + Delete effect: Â¬have(person, pizza)
# + Add effect: full(person)

# +
#   #+begin_src scheme
#   (:action eat :parameters (?person ?pizza)
#   #+end_src
# +
#   #+begin_src scheme
#   ã€€ :precondition (and (not (full ?person))
#   ã€€                   (have ?person ?pizza))
#   #+end_src
# +
#   #+begin_src scheme
#   ã€€ :effects (and (not (have ?person ?pizza)) ; delete effects
#   ã€€               (full ?person)))           ; add effects
#   #+end_src

+
  #+begin_quote
  _/çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«:/_ $z_{t+1} = (z_t \setminus \text{del}(a)\ )\ \cup\ \text{add}(a)$
  #+end_quote

  åŠ¹æœ $\text{del}(a), \text{add}(a)$ ã¯ *$z_t$ ã«ä¾å­˜ã—ãªã„*.

+ */ã‚ˆãã‚ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«/* ã¯ *ãã“ã‚’ã‚ã‹ã£ã¦ãªã„ï¼* (è‡ªå‹•è¨ˆç”»ã®äººãŒã„ãªã„)
  #+begin_quote
  */ã‚ˆãã‚ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«:/* $z_{t+1} = \text{ãªãã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ}(z_t, a)$
  #+end_quote

+

  #+begin_larger
  #+begin_center
  PDDLã«å¤‰æ›ã§ããªã„ â†’ */é«˜é€Ÿãªã‚½ãƒ«ãƒã‚’ä½¿ãˆãªã„/*
  #+end_center
  #+end_larger


# +
#   #+begin_quote
#   _/PDDL effect:/_ $z_{t+1} = (z_t \ \setminus \ \text{del}(a)\ )\ \cup\ \text{add}(a)$
#
#   */AAE's effect:/* $z_{t+1} = \text{apply}(z_t, a)$
#   #+end_quote
#
#   *In PDDL, effects should be consistent for each action*.
#
#   #+begin_alignright
#   *= Effects should be independent from $z_t$*
#   #+end_alignright

* é«˜é€Ÿã‚½ãƒ«ãƒã®ãŸã‚ã«PDDLè¨˜å·ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ @@html:<br>@@ â†’è¨€èªã®åˆ¶ç´„ã‚’ */NNãƒ¢ãƒ‡ãƒ«ã«åŸ‹ã‚è¾¼ã¾ãªã„ã¨ã„ã‘ãªã„/*

+ *PDDLã§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŠ¹æœã¨çŠ¶æ…‹ $z_t$ ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹.*

#+begin_center
+ â†’ $z_t$ ã¨ç‹¬ç«‹ãªå¤‰æ•° *e* ã‚’å°å…¥ã™ã‚‹å¿…è¦ã‚ã‚Šã€‚

+ ã€€
  
  [[png:space-ae/graphical-model-csae]]
#+end_center

# #+begin_smaller
# ç„¡åã‚·ãƒ³ãƒœãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€$z_t$ ã‚„ $a$ ã«ã¯ *é›¢æ•£éš ã‚Œå¤‰æ•°* ã‚’ä½¿ã† (Gumbel-Softmax)
# #+end_smaller

+ ã“ã‚Œã‚’ *é›¢æ•£çš„éš ã‚Œå¤‰æ•°* (Gumbel-Softmax) + *å¤‰åˆ†å­¦ç¿’* ã§è¨“ç·´ã€‚

  ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒæ•°å­¦çš„æ ¹æ‹ ã‚’ä¸ãˆã‚‹

* é›¢æ•£è¡¨ç¾å­¦ç¿’: Gumbel-Softmax VAE ã»ã‹

[[png:sae/state-ae-before]]

** é›¢æ•£è¡¨ç¾å­¦ç¿’: Gumbel-Softmax VAE ã»ã‹

[[png:sae/state-ae]]

+ 2017å¹´ãã‚‰ã„ä»¥é™ã€å¤šæ•°ã®æ‰‹æ³•ãŒè€ƒæ¡ˆã•ã‚Œã¦ã„ã‚‹

  (REBAR, RELAX, VQVAE, SQVAE, ...)

  +
    #+begin_alignright
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ã‚’ã™ã‚‹ãªã‚‰è¦ãƒã‚§ãƒƒã‚¯!
    #+end_alignright

* Important aspects of symbol grounding:                           :noexport:

Definition of symbol: *not* just state variables.

*Symbol* : A structure containing a pointer to a /name/ and a pointer to an /object/.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_src lisp
(defstruct symbol
  name
  object)
#+end_src
#+end_span4
#+begin_span4
#+begin_src python
class Symbol:
  name: string,
  obj: Any
#+end_src
#+end_span4
#+begin_span4
#+begin_src C
struct symbol {
    void* name;
    void* object;
}
#+end_src
#+end_span4
#+end_row-fluid
#+end_container-fluid

A symbol may point to *various things* through a hash table.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_src lisp
(symbol-value    'my-variable)
(symbol-function 'my-function)
(make-instance   'my-class)
#+end_src
#+end_span6
#+begin_span6
#+begin_src python
function_dictionary["myfunc"]
function_dictionary["myfunc"]
#+end_src
#+end_span6
#+end_row-fluid
#+end_container-fluid

* /ã‚·ãƒ³ãƒœãƒ«/ ã¯ä¸€ç¨®é¡ã ã‘ã§ã¯ãªã„

*/å‘½é¡Œã‚·ãƒ³ãƒœãƒ«/* (handempty, ...) vs. _/ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ³ãƒœãƒ«/_ (pickup,...).

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:space-ae/graphical-model-csae-only]]
#+end_span4
#+begin_span8
#+begin_center
+
  #+begin_larger
  ã“ã‚Œã‚‰ã¯ *åˆ¥ã®åå‰ç©ºé–“ã«å­˜åœ¨ã™ã‚‹*.
  #+end_larger

  ã€€

  è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«ã‚‚åŒæ§˜:

  ã€€ I */like/* an apple : å‹•è©

  ã€€ Thanks for the */like/* on Facebook! : åè©

  Common Lisp (Lisp-2) vs. Scheme (Lisp-1)
#+end_center

#+begin_larger
#+begin_alignright
+ â†’ *åˆ¥ã®åå‰ç©ºé–“* ã«ã‚ã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯

  *åˆ¥ã®è¨€èªåˆ¶ç´„ãŒå¿…è¦ã€‚*
#+end_alignright
#+end_larger
#+end_span8
#+end_row-fluid
#+end_container-fluid

* PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/1]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/2]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/3]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/4]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/5]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

*ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ³ãƒœãƒ«:* ã€Œåˆ¥ã®ã‚²ãƒ¼ãƒ ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€ â†’ åˆ¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

ã€€ãƒãƒªã‚ªã€ã‚¼ãƒ«ãƒ€ã€ãƒ‰ãƒ³ã‚­ãƒ¼ã‚³ãƒ³ã‚°ã€F-Zero ...

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/6]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

*ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ³ãƒœãƒ«:* ã€Œåˆ¥ã®ã‚²ãƒ¼ãƒ ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€ â†’ åˆ¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

ã€€ãƒãƒªã‚ªã€ã‚¼ãƒ«ãƒ€ã€ãƒ‰ãƒ³ã‚­ãƒ¼ã‚³ãƒ³ã‚°ã€F-Zero ...


* å®Ÿéš› Latplan ã‚’è¿°èªè«–ç†ã«æ‹¡å¼µå¯ (ICLR2021 workshop)

*Latplan/FOSAE++* : ç”»åƒå†…ã®ç•°ãªã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä½¿ãˆã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å­¦ç¿’

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:graphical-model-fosae++-nopatch2]]
#+end_span4
#+begin_span8
[[png:blocks-wide]]
#+end_span8
#+end_row-fluid
#+end_container-fluid


+ *è¿½åŠ åˆ¶ç´„:* Successor State Axiom (*éæŸç¸›å¤‰æ•°ã®ç¦æ­¢* [Reiter, 1991])

+ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: $a=\text{Eat}(\text{pizza}ğŸ•)$ : å¼•æ•° $\text{pizza}ğŸ•$
+
  #+begin_center
  *åŠ¹æœ:* */ç™ºè¡¨(ãƒ•ãƒ­ãƒ , ã‚¢ãƒ¼ãƒãƒ¼ãƒ‰ã‚³ã‚¢ã®æ–°ä½œ)/*
  #+end_center
+
  #+begin_alignright
  â†‘ $\text{pizza}ğŸ•$ ã‚’ä½¿ã‚ãªã„ã®ã§ç¦æ­¢
  #+end_alignright

+
  #+begin_larger
  #+begin_center
  æ–°ãŸãªåˆ¶ç´„ãŒ *è¿°èªã‚·ãƒ³ãƒœãƒ«* ã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¯èƒ½ã«ã—ãŸ
  #+end_center
  #+end_larger


* çµè«–                                                             :noexport:

#+begin_xlarge
è¨˜å·æ¥åœ°ã—ãŸã„ãªã‚‰

#+begin_center
*å¯¾è±¡è¨€èªã‚’ /ã¡ã‚ƒã‚“ã¨/ ç†è§£ã—ã¦*
#+end_center

#+begin_alignright
*/æ­£ã—ã„/ ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚ã†!*
#+end_alignright
#+end_xlarge




* è¨˜å·çš„è¨€èªã«æ¥åœ°ã—ãŸã„ãªã‚‰ã€å¯¾è±¡è¨€èªã‚’ã¡ã‚ƒã‚“ã¨ç†è§£ã—ã¦ */æ­£ã—ã„/* ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚Šã¾ã—ã‚‡ã† :noexport:

ä¾‹1:
#+begin_center
ç”»åƒå…¥åŠ› â†’ PDDL â†’ é«˜é€ŸPDDLã‚½ãƒ«ãƒ
#+end_center

ä¾‹2 (æœªç™ºè¡¨):
#+begin_center
+ éç·šå½¢çŠ¶æ…‹é·ç§»ãƒ¢ãƒ‡ãƒ« â†’ ç·šå½¢éš ã‚Œåˆ¶å¾¡ãƒ¢ãƒ‡ãƒ« â†’ é«˜é€Ÿç·šå½¢åˆ¶å¾¡ã‚½ãƒ«ãƒ
#+end_center

[[png:space-ae/graphical-model-style]]

#+begin_center
+ *Caution: It requires a /deep/ understanding of your formalism*

  *(e.g., PDDL, linear control).* Know what you are modeling!
#+end_center

* Extending the generative model (unpublished)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
+ Lifted actions,

  Parameters *x*

  [[png:space-ae/graphical-model-fosae]]

  Predicates, too.

  *Compatible with First Order Logic.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Temporal actions,

  Action durations *d*

  [[png:space-ae/graphical-model-time]]

  *Compatible with LTL.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Domain symbol *D*,

  Problem symbol *P*

  [[png:space-ae/graphical-model-domain]]

  Allows mixed-domain training?
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

# + Discrete-Continuous
#
#  [[png:space-ae/graphical-model-style]]



* Part 1: Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ *Goal: ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ*

  [[sgif:mars/yoda-hmph]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ *Latplan* ã§è¨˜å·æ¥åœ°

  [[sjpg:puzzle]]

#+end_center
#+end_span5
#+end_row-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ ç•°ãªã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯  *åˆ¥ã€…ã®åˆ¶ç´„ãŒå¿…è¦*

  [[png:namespace/6]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦

  *åˆ¶ç´„ã‚’ç²¾ç·»ã«è¨˜è¿°å¯èƒ½*

  [[png:space-ae/graphical-model-csae]]
#+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

# #+begin_center
# Masataro Asai, Hiroshi Kajino, Alex Fukunaga, Christian Muise:
#
# *Classical Planning in Deep Latent Space.* Arxiv 2107.00110 -> JAIR
# #+end_center

+ è¨˜å·æ¥åœ°ã™ã‚‹ãªã‚‰ *å¯¾è±¡è¨€èªã‚’ /ã¡ã‚ƒã‚“ã¨/ ç†è§£ã—ã¦ /æ­£ã—ã/ ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã‚ˆã†!*


#+begin_center
è³ªå•: 2ã¤ã¾ã§
#+end_center

* Why _/action symbol grounding/_ is necessary?                    :noexport:

"designers always know the action space"? */Wrong!/*

Humans have a good understanding of low-level actuations...

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+
  #+begin_center
  Running
  #+end_center
  [[sjpg:triathlon/running]]
# sjpg:triathlon/bicycle
# sjpg:triathlon/swimming
#+end_span6
#+begin_span6
+
  #+begin_center
  Career Path
  #+end_center
  [[sjpg:triathlon/career]]

  #+begin_center
  Do you fully understand all of your opportunities?
  #+end_center
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
+ *Low-level Control*
#+end_center
#+end_span6
#+begin_span6
#+begin_center
+ */High-level Action/*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid



* Part 2:

#+begin_xlarge
ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®
#+begin_center
*/ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£/*
#+end_center
#+begin_alignright
ã‚’è€ƒãˆã‚‹
#+end_alignright
#+end_xlarge

#+begin_alignright
+ â†’ *è¡¨ç¾å­¦ç¿’ä»¥å¤–* ã® ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°+æ©Ÿæ¢°å­¦ç¿’
#+end_alignright


* ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/6]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/5]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/4]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/3]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/2]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/1]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0a]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* æ¢ç´¢ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã®å­¦ç¿’:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_xlarge
#+begin_center
RL

(ICAPS 2022)
#+end_center
#+end_xlarge
#+end_span6
#+begin_span6
#+begin_xlarge
#+begin_center
SL

(IJCAI 2024)
#+end_center
#+end_xlarge
#+end_span6
#+end_row-fluid
#+end_container-fluid

* ICAPS22

[[png:pddlrl/title]]

* Value function learning

SGD Loss function:

\[
\frac{1}{|B|}\sum_{s\in B} \frac{1}{2}\left| V(s) - \mathbb{E}_{a\in A} [Q(a, s)] \right|^2
\]

$B$: batches in the experience replay

#+begin_larger
Research Questions:
#+begin_center
+ *Q: RLã ã‘ã§ å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’è§£ã‘ã‚‹ã‹?* (A: *ä¸å¯*)

+ *Q: RLã ã‘ã§ ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã‹?* (A: *ä¸å¯*)

+ *Q: ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®é“å…·ã¨é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚Œã°?* */(A: å¯)/*
#+end_center
#+begin_alignright
+ Lessons learned: *å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ç”˜ãè¦‹ã‚‹ãª*
#+end_alignright
#+end_larger


* Why Classical Plannning is difficult for RL?

Rewards in Classical Planning:

#+begin_center
#+begin_larger
+ $r(s,a,s') = \left\{\begin{array}{ll} 1 & \text{if}\ s' \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  *This is extremely sparse.*

#+end_larger
#+end_center

+ */Any non-goal states/* are *equally /worthless/*

  + *No guidance* for RL until a goal,

  + which is *difficult to find* in the first place!

* Potential-Based */Reward Shaping/* (PBRS) @@html:<br>@@ for Sparse Rewards

Given a *potential function* $\phi(s)$, */redefine rewards/* as:

\begin{align}
\hat{r}(s, a, s') &= r(s, a, s') + \gamma \phi(s') - \phi(s). \label{eq:shaping1}
\end{align}

+ */Important:/* PBRS *preserves* the optimal value function

  \begin{align}
  V^*_\gamma(s) &= \hat{V}^*_\gamma(s) + \phi(s). \label{eq:shaping2}
  \end{align}

  #+begin_alignright
  #+begin_larger
  = *learning the residual from $\phi(s)$*
  #+end_larger
  #+end_alignright

#+begin_note
Ng, Harada, Russell. "Policy invariance under reward transformations: Theory and application to reward shaping." ICML. Vol. 99. 1999.
#+end_note

* Heuristic Function $h(s)$ in classical planning

+ *Distance estimate*
  + $h(s)=5 \quad \longleftrightarrow \quad s - s_1 - s_2 - s_3 - s_4 - \text{goal}$ (ãŠãŠã‚ˆã) 
  + Many implementations: $h^{\text{add}}$, $h^{\text{FF}}$, $h^{\text{CEA}}$, ...

    #+begin_smaller
    (Bonet&Geffner 01, Hoffmann 01, Helmert 08) ã‚‚ã£ã¨æ–°ã—ã„ã®ã‚‚ã‚ã‚‹
    #+end_smaller

#+begin_xlarge
+ *Heuristics satisfies the PBRS requirements*

+ 
  #+begin_alignright
  */Let's use $h(s)$ for PBRS!/*
  #+end_alignright
#+end_xlarge

* Issue 1: */Discounting/*

#+begin_larger
*RL : /discounted rewards/* â†” *Planning :* _/undiscounted costs/_
#+end_larger

ã€€

Our solution:

+ *Training* : Convert $h(s)\rightarrow h_{\gamma}(s)$ : */discounting/* $h(s)$ for PBRS

+
  \begin{align}
  \phi(s) &= h_\gamma(s) = \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.
  \end{align}

  # V_\gamma(s) &= \hat{V}_\gamma(s) + \phi(s) \quad \text{where}\\
  # \phi(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot (-1) = -\frac{1-\gamma^{h(s)}}{1-\gamma}
  # h(s) &= \sum_{t=1}^{h(s)} 1, & h_\gamma(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.


+ *Planning* : Convert $V_{\gamma}(s)\rightarrow V(s)$ : _/undiscounting/_ a learned $V_{\gamma}(s)$

+   
  \begin{align}
  V_\gamma(s) = \dfrac{1 - \gamma^{V(s)}}{1 - \gamma}
  &\Leftrightarrow
  V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s)).
  \end{align}

* Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm6]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm5]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm4]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm3]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm2]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm1]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm]]

+ *Size* & *Permutation invariant* to FOL binary inputs

+ *Quite useful for various FOL tasks*
  + ILP [Dong et. al. ICLR 2019]
  + Regression (our work) 

* Results

Skipped

* ICAPS 2022 : Conclusion

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *Sparse Rewards*

  #+begin_smaller
  $r(s) = \left\{\begin{array}{ll} 1 & \text{if}\ s \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  ã‚´ãƒ¼ãƒ«ã«ãŸã©ã‚Šç€ãã®ãŒ

  ã¨ã¦ã‚‚é›£ã—ã„
  #+end_smaller
#+end_span6
#+begin_span6
+ *Potential-Based*

  *Reward Shaping*

  $V^*_\gamma(s) = \hat{V}^*_\gamma(s) + \phi(s)$

  #+begin_smaller
  $\phi(s)=$ è·é›¢ã®è¦‹ç©ã‚‚ã‚Š $h(s)$

  Sparse Reward ã‚’è§£æ±º
  #+end_smaller
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Discounting*

  $h_\gamma(s)  = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}$

  $V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s))$

  #+begin_smaller
  ã“ã‚ŒãŒãªã„ã¨

  $h(s)=\infty$ ã®ã¨ãæ­»ã¬
  #+end_smaller
#+end_span6
#+begin_span6
+ *First-Order Logic Input*

  [[png:pddlrl/nlm]]

  #+begin_smaller
  ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®å•é¡Œã«æ±åŒ–
  #+end_smaller

#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_smaller
+ *Q: RLã ã‘ã§ å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’è§£ã‘ã‚‹ã‹?* (A: *ä¸å¯*)

  *Q: RLã ã‘ã§ ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã‹?* (A: *ä¸å¯*)

  *Q: ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®é“å…·ã¨é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚Œã°?* */(A: å¯)/*

  Lessons learned: */å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ç”˜ãè¦‹ã‚‹ãª/*
#+end_smaller
#+end_center


* IJCAI24

[[png:truncated-gaussian/title]]



#+begin_xlarge
#+begin_center
RL å«Œã„ãªã‚“ã§ ... æ•™å¸«ã‚ã‚Šå­¦ç¿’!
#+end_center
#+end_xlarge

* Task: Learn the shortest path cost

[[png:truncated-gaussian/1b]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1a]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1]]

#+begin_larger
#+begin_center
+ *Q: Can we exploit the /admissibility/ of heuristics* in training?
#+end_center
#+end_larger

* Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2j]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2i]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2h]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2g]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2f]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2e]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2d]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2c]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2b]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2a]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2]]

* 

[[png:truncated-gaussian/3c]]

** 

[[png:truncated-gaussian/3b]]

** 

[[png:truncated-gaussian/3a]]

** 

[[png:truncated-gaussian/3]]

* Thinking with Distributions                                      :noexport:

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

+ We know $h^*\ge 0$
+ We know $h^*\ge h$ : *admissible heuristics*
+ *Why the heck* do we assign */non-zero probability/* to $h^*< 0$?
+ i.e. $\mathcal{N}(\mu,\sigma)$ *ignores* our */expert knowledge/* on $h^*$
+ It is not the *correct* distribution

* Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4e]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4d]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4c]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4b]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4a]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

* What is the correct distribution?

+ Follow the */Maximum Entropy Principle/* (Jaynes 1957):

  #+begin_quote
  + /Choose the distribution with the *largest entropy*/
  + /among those that satisfy the *expert knowledge / constraint*./
  #+end_quote

+
  #+begin_alignright
  #+begin_larger
  â†ª We know a lower bound $h\le h^*$ !

  *This is our expert knowledge!*
  #+end_larger
  #+end_alignright

* What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5c]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5b]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5a]]

** Results 

[[png:truncated-gaussian/6]]

* IJCAI 2024 : Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *We learn* $h^*$

  [[png:truncated-gaussian/1]]

#+end_span6
#+begin_span6
+ *Square error = Gaussian*

  [[png:truncated-gaussian/3]]
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Gaussian is a lie*

  [[png:truncated-gaussian/4]]
#+end_span6
#+begin_span6
+ *Max-ent: Truncated Gaussian*

  [[png:truncated-gaussian/5a]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

Take-home message: *Whenever you see a square error, doubt it!*

* ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0b]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_xlarge
#+begin_center
MCTS + Gaussian Bandit

(ECAI 2024)
#+end_center
#+end_xlarge
#+end_span6
#+begin_span6
#+begin_xlarge
#+begin_center
MCTS + Power Bandit

(SoCS 2024 extended abstract)
#+end_center
#+end_xlarge
#+end_span6
#+end_row-fluid
#+end_container-fluid

* ECAI 2024

[[png:scale-invariant/title]]

* How Traditional Algorithms work : Dijkstra (1956)                :noexport:

dfs bfs *dijkstra* A* GBFS

[[png:search/astar2]]

Dijkstra: *priority queue sorted by* $g(n)$


** How Traditional Algorithms work : Dijkstra (1956)

dfs bfs *dijkstra* A* GBFS

[[png:search/astar3]]

Dijkstra: *priority queue sorted by* $g(n)$


** How Traditional Algorithms work : Dijkstra (1956)

dfs bfs *dijkstra* A* GBFS

[[png:search/astar4]]

Dijkstra: *priority queue sorted by* $g(n)$



** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar5]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar6]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */A*/* (Hart et. al, 1968)

dfs bfs dijkstra */A*/* GBFS

[[png:search/astar7]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs0]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs1]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs2]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$



** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/gbfs3]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Greedy Best First Search/* (Geffner et. al, 2001)

dfs bfs dijkstra A* *GBFS*

[[png:search/astar7]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$


** How Traditional Algorithms work : */Hill Climbing/* in RL

dfs bfs dijkstra A* *GBFS*

[[png:search/hc1]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

** How Traditional Algorithms work : */Hill Climbing/* in RL

dfs bfs dijkstra A* *GBFS*

[[png:search/hc2]]

Dijkstra: *priority queue sorted by* $g(n)$

*/A*/* : *priority queue sorted by* $f(n) = g(n)+h(n)$

*/GBFS/* : *priority queue sorted by* $h(n)$ : Ignore $g(n)$

*Importance of Priority Queue!!!*

* Monte Carlo Tree Search (General version)

[[png:search/mcts01]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts02]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts03]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts04]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts05]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts06]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts07]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ |
|-------------+----------------------+----------------------|
| Game        | âœ— Hand-coded         |                      |
| (e.g., Go)  | âœ— Not accurate       |                      |
|-------------+----------------------+----------------------|
| ã€€ã€€ã€€ã€€ã€€ã€€ | ã€€                    | ã€€                    |
| ã€€ã€€ã€€ã€€ã€€ã€€ | ã€€                    | ã€€                    |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|-------------+----------------------+------------------------|
| Game        | âœ— Hand-coded         | âœ” Finite horizon (9x9) |
| (e.g., Go)  | âœ— Not accurate       |                        |
|-------------+----------------------+------------------------|
| ã€€ã€€ã€€ã€€ã€€ã€€ | ã€€                    | ã€€                      |
| ã€€ã€€ã€€ã€€ã€€ã€€ | ã€€                    | ã€€                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|------------+--------------------+----------------------|
| Game       | âœ— Hand-coded       | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate     | âœ” Dense reward       |
|------------+--------------------+----------------------|
| ã€€ã€€ã€€ã€€ã€€ã€€     | ã€€                  | ã€€                    |
| ã€€ã€€ã€€ã€€ã€€ã€€     | ã€€                  | ã€€                    |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | âœ— Hand-coded         | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate       | âœ” Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      |                      |
| Planning   |                      |                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | âœ— Hand-coded         | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate       | âœ” Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      | âœ— Infinite horizon   |
| Planning   |                      |                      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | âœ— Hand-coded         | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate       | âœ” Dense reward       |
|------------+----------------------+----------------------|
| Classical  |                      | âœ— Infinite horizon   |
| Planning   |                      | âœ— Sparse reward      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | Heuristic function ã€€ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | âœ— Hand-coded         | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate       | âœ” Dense reward       |
|------------+----------------------+----------------------|
| Classical  | âœ” Automated          | âœ— Infinite horizon   |
| Planning   |                      | âœ— Sparse reward      |

** Monte Carlo Tree Search (General version)

[[png:search/mcts08]]

| ã€€ã€€ã€€ã€€ã€€ã€€ | */Heuristic function/* ã€€ | Simulation / Rollout   |
|------------+----------------------+----------------------|
| Game       | âœ— Hand-coded         | âœ” Finite horizon (9x9)     |
| (e.g., Go) | âœ— Not accurate       | âœ” Dense reward       |
|------------+----------------------+----------------------|
| Classical  | âœ” Automated          | âœ— Infinite horizon   |
| Planning   | âœ” Domain-Independent | âœ— Sparse reward      |

* Bandit for Action Selection                                      :noexport:

[[png:bandit/1]]

** Bandit for Action Selection

[[png:bandit/2]]

** Bandit for Action Selection

[[png:bandit/3]]

* Backup in MCTS : Avg

[[png:tree-based/1]]

** Backup in MCTS : Avg vs. Min

[[png:tree-based/2]]


** Backup in MCTS : Avg vs. Min = */GBFS, A*/*

[[png:tree-based/3]]

#+begin_center
#+begin_xlarge
+ *GBFS: Traditional SotA*
+ UCT has been _/bad/_

  (UCT = MCTS+UCB1)
#+end_xlarge
#+end_center

#+begin_note
Schulte and Keller. "Balancing exploration and exploitation in classical planning." SOCS, 2014
#+end_note

** GBFS */can/* be seen as MCTS (Keller et al. 2013)               :noexport:

|   |           |                |                                       <r> |
|   | Algorithm |                |                          Action Selection |
|---+-----------+----------------+-------------------------------------------|
|   | */GBFS/*  | */MCTS + Min/* |          $\hspace{3em}h(s)\hspace{6.5em}$ |
|   | GreedyUCT | MCTS + Avg     | $\hspace{3em}h(s)-c\sqrt{{\log N}/{n_i}}$ |

#+begin_xlarge
#+begin_center
+ MCTS+UCB1 < */MCTS+Min/*
#+end_center
#+end_xlarge

* Search / Planning community is SO bad at statistics (more wrong than we initially thought) :noexport:

*Nobody* understands the original UCB paper [Auer 2002] properly. ğŸ˜¢

+ [Keller 2013] uses UCB1 in [Auer 2002], which assumes [0, 1] rewards
  + *BUT!* Node evaluation is a distance to the goal, which is *unbounded*
  + To satisfy [0,1], normalize $h(s_i)$ by min, max over arms (hack! ğŸ˜ )
+ [Auer 2002] *already* proposed *UCB1-Normal* (Theorem 4)
  + UCB1 for the unbounded rewards == Gaussian distributed rewards
+ [Tesauro 2010] *rediscovers* UCB1-Normal, *but does not realize it! ğŸ¤¯*
  + UCB1 was widely known in 2010, but no one knew UCB1-Normal ğŸ¤”

#+begin_note
Keller, T.; and Helmert, M. 2013. Trial-Based Heuristic Tree Search for Finite Horizon MDPs. ICAPS

Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-Time Analysis of the Multiarmed Bandit Problem. Machine Learning

Tesauro, G.; Rajan, V.; and Segal, R. 2010. Bayesian Inference in Monte-Carlo Tree Search. UAI
#+end_note

* Why: */Poor theoretical understanding/* of bandits.

#+begin_quote
$\text{UCB1}_i = \mu_i-c\sqrt{\frac{\log N}{n_i}}$ , *for* $\mu_i \in [0,c]$

#+begin_center
$N$: parent visit, $n_i$: visit for child $i$

$\mu_i$ : mean over the subtree of $i$
#+end_center
#+end_quote

+ *Rewards must have a /known, fixed, shared reward range/*.

  + But $h(s)$ *have /no known range!/*

    _Requirments unsatisfied. All theoretical guarantees violated._

+ *Our solution: Gaussian Bandit* defined on *Entire* $\mathbb{R}=[-\infty,\infty]$

  #+begin_quote
  $\text{UCB1-Normal2}_i = \mu_i-\sigma_i \sqrt{\log N}$ , *for* $\mu_i \in \mathbb{R}$

  #+begin_center
  $\mu_i, \sigma_i$ : mean / stdev over the subtree of $i$
  #+end_center
  #+end_quote

* Results (ECAI 2024)

[[spng:ecai24-results]]

* Avg vs. Min                                                      :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/5]]
#+end_span6
#+begin_span6
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
#+end_span6
#+begin_span6
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
+ *Least average* :: *on average* close to the goal
  + Bad nodes can *hide* goals!
#+end_span6
#+begin_span6
[[png:tree-based/6]]
+ *Least minima (preferred)* :: *at best* close to the goal
  + Bad nodes do not matter!
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Avg vs. Min               :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:tree-based/4]]
#+end_span6
#+begin_span6
Select a subtree by...

+ *Least average* :: *on average* close to the goal
  + Bad nodes can *hide* goals!
+ *Least minima (preferred)* :: *in the best case* close to the goal
  + Bad nodes do not matter!
#+end_span6
#+end_row-fluid
#+end_container-fluid


Characterized by the *backup strategy difference*.

* Avg. vs. Min. = Different Backup strategy                        :noexport:

+ Standard UCB1-Normal --- avg ::
  Backup $(\sum_i n_i,\ \frac{\sum_i n_i \mu_i}{\sum_i n_i})$ : weighted sum of means
+ Keller et al. 2013 --- */minimum of means/* without statistical justification ::
  Backup $(\sum_i n_i,\ \min_i \mu_i)$
+ Tesauro et al. 2010 --- */approximate minimum/* ::
  Backup a dist. of minimum of Gaussians [Clerk, 1961]
  # + If $X_1\sim \mathcal{N}(\mu_1,\sigma_1), X_2\sim \mathcal{N}(\mu_2,\sigma_2)$, then
  # + $\max(X_1,X_2)\sim \mathcal{N}(\mu_3,\sigma_3)$ where
  # + $\mu_3=\mu_1\Phi(\alpha)+\mu_2\Phi(-\alpha)+(\sigma_1^2+\sigma_2^2)\phi(\alpha)$
  # + $\sigma_3=(\mu_1^2+\sigma_1^2)\Phi(\alpha)+(\mu_2^2+\sigma_2^2)\Phi(-\alpha)+(\mu_1+\mu_2)(\sigma_1^2+\sigma_2^2)\phi(\alpha)$
  # + *Cons*: Not exact for $>2$ arms, *Pros*: Statistical Justification

#+begin_quote
#+begin_center
#+begin_xlarge
None outperforms GBFS
#+end_xlarge
in our reproduction
#+end_center
#+end_quote

* Experiments                                                      :noexport:

|                                                                              <r> |                                       |                |              |
| /                                                                                | <                                     |                | >            |
| selection \ backup                                                               | $\frac{\sum_i n_i \mu_i}{\sum_i n_i}$ | $\min_i \mu_i$ | Tesauro      |
|----------------------------------------------------------------------------------+---------------------------------------+----------------+--------------|
|                                                                          $\mu_i$ |                                       | GBFS           |              |
| $\frac{\mu_i-\min_j h(s_j)}{\max_j h(s_j)-\min_j h(s_j)}-c\sqrt{{\log N}/{n_i}}$ | Bounded UCB                           | Bounded UCB*   | Bounded UCB+ |
|                                                   $\mu_i-c\sqrt{{\log N}/{n_i}}$ | UCB                                   | UCB*           | UCB+         |
|                                                   $\mu_i-\sigma_i \sqrt{\log N}$ | UCB1-Normal                           | UCB1-Normal*   | UCB1-Normal+ |

There are also Std FF vs Rnd FF versions, $c\in\{0.3,1.0,3.0\}$

Too many, â†’ *We focus on UCB1-Normal*

** Results vs. GBFS, deterministic FF : not conclusive

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
*Std FF, mean (win)*

[[spng:plots/expansions__UCT-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
Std FF, min

[[spng:plots/expansions__UCTStar-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Std FF, tesauro (win)*

[[spng:plots/expansions__UCTPlus-False-10-0.2-inf-0.5-False__GBFSMean-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results vs. GBFS with mean of randomized FF

Since GBFS + mean randFF < GBFS + min randFF, this is a straw-man comparison

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT mean (win)*

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT min (win)*

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
*Rnd FF, GBFS mean vs UCT tesauro (win)*

[[spng:plots/expansions__UCTPlus-True-10-0.2-inf-0.5-False__GBFSMean-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results vs. GBFS with min of randomized FF

UCB1-Normal, UCB1-Normal*, UCB1-Normal+ are *worse* than GBFS with min of randomized FF!

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT mean (lost)/

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT min (lost)/

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+begin_span4
#+begin_center
/Rnd FF, GBFS min vs UCT tesauro (lost)/

[[spng:plots/expansions__UCTPlus-True-10-0.2-inf-0.5-False__GBFSMin-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Results: Tesauro vs mean, min (not conclusive)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
Std FF, vs mean

[[spng:plots/expansions__UCT-False-10-0.2-inf-0.5-False__UCTPlus-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+begin_span6
#+begin_center
*Std FF, vs min (win)*

[[spng:plots/expansions__UCTStar-False-10-0.2-inf-0.5-False__UCTPlus-False-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
/Rnd FF, mean (lost)/

[[spng:plots/expansions__UCT-True-10-0.2-inf-0.5-False__UCTPlus-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+begin_span6
#+begin_center
Rnd FF, min

[[spng:plots/expansions__UCTStar-True-10-0.2-inf-0.5-False__UCTPlus-True-10-0.2-inf-0.5-False]]

#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid


** Conclusion of reproducing existing work

* New Algorithm : Model the minimum */correctly/*

Fit the data to...

|             |                                         |        <c>         |
| UCB1        | *a /known/ finite support distribution* | $[0,1]$ , $[3,5]$  |
| UCB1-Normal | *a Gaussian distribution*               | $[-\infty,\infty]$ |

#+begin_quote
+ *Do heuristics have /known/ bounds e.g. $[3,5]$ ? /No!/*

+ *Are heuristic samples $\in[-\infty,\infty]$ ? /No! It is non-negative!/*

  $0 \ge h(s)$

#+begin_larger
#+begin_center
+ *Gaussian assumption is /STILL WRONG/*
#+end_center
#+end_larger

#+end_quote

# + *Fact* : Heuristic $h^{\text{FF}}$ has */unknown/* bounds
# 
#   \[
#   0 \leq h^+(s) \leq h^{\text{FF}}(s) < \ ?
#   \]
# 
#   (NP-hard to compute $h^+$ == unknown, $?$ is also unknown)

* What distribution is the right answer?

|             | Range            | How it is distributed?          |
|-------------+------------------+---------------------------------|
| Gaussian    | _/Unbounded/_ âœ—  |                                 |
| Exponential | */Half-Bounded/* |                                 |
| Uniform     | */Bounded/*      |                                 |
|             |                  |                                 |
|             |                  | ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ |


[[png:scale-invariant/candidate-distribution]]

** What distribution is the right answer?

|             | Range            | How it is distributed?              |
|-------------+------------------+-------------------------------------|
| Gaussian    | _/Unbounded/_ âœ—  |                                     |
| Exponential | */Half-Bounded/* | _/Smallest value is most common/_ âœ— |
| Uniform     | */Bounded/*      | Equally likely around min and max   |
|             |                  |                                     |
|             |                  | ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€     |

[[png:scale-invariant/candidate-distribution]]

** What distribution is the right answer?

|             | Range            | How it is distributed?              |
|-------------+------------------+-------------------------------------|
| Gaussian    | _/Unbounded/_ âœ—  |                                     |
| Exponential | */Half-Bounded/* | _/Smallest value is most common/_ âœ— |
| Uniform     | */Bounded/*      | Equally likely around min and max   |
| *Power*     | */Bounded/*      | */Smallest value is rare/*          |
|             |                  | ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€     |

[[png:scale-invariant/power]]

** What distribution is the right answer?                          :noexport:

*Power* distribution : *Bounded* distribution, with *largest point = /rarest/*

Sign flip ($\min \leftrightarrow \max$) will achieve the desired effect

[[spng:gpd]]

* But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

** But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

*Extreme Value Theorem type 1*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
 M_n= \max     \frac{x_i-b_n}{a_n}  \longrightarrow \mathrm{EVD}(\mu, \sigma, \xi)
\]

** But Why? */What theory/* justifies Power?

*Central Limit Theorem*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma$

\[
 S_n= \sum_{i=0}^n \frac{x_i-b_n}{a_n}  \longrightarrow \mathcal{N}(\mu, \sigma)
\]

*Extreme Value Theorem type 1*: Given i.i.d. RVs $x_0, \ldots x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
 M_n= \max     \frac{x_i-b_n}{a_n}  \longrightarrow \mathrm{EVD}(\mu, \sigma, \xi)
\]

*/Extreme Value Theorem type 2/*: Given i.i.d. RVs $x_0 < \ldots < x_k < \ldots < x_n$, there is a sequence $a_n>0, b_n$ s.t. $\exists \mu, \sigma, \xi$

\[
  p(x | x > x_k)  \longrightarrow \mathrm{GPD}(\mu, \sigma, \xi)
\]

$\mathrm{GPD}$ : */Generalized Pareto Distribution/* is the way to go. *Why?*

** Power, Uniform, Exponential, Pareto are special cases of Generalized Pareto $\mathrm{GPD}(\theta, \sigma, \xi)$.

|                           |   <c>    |             <c>             |
|                           |   min    |             max             |
|---------------------------+----------+-----------------------------|
| $\xi>0$ = Pareto          | $\theta$ |          $\infty$           |
| $\xi=0$ = Exponential     | $\theta$ |          $\infty$           |
| $\xi<0$ = (Flipped) Power | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |
| $\xi=-1$ = Uniform        | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |
| $\xi<-1$                  | $\theta$ | $\theta-\frac{\sigma}{\xi}$ |

[[spng:gpd]]

* Modeling the average

[[png:clt-vs-evt/0]]

** Modeling the average = Fitting the dist. with Gaussian

[[png:clt-vs-evt/1]]

** Modeling the maximum

[[png:clt-vs-evt/2]]

** Modeling the maximum = Fitting the tail dist. with GP

[[png:clt-vs-evt/3]]

* New Algorithm : Model the minimum */correctly/*

Fitting GPD itself is *difficult* (computational issue)

\rightarrow fit subclasses: *Uniform* and *Power*

|                | Lower bound        | Upper bound     | Shape               |
|----------------+--------------------+-----------------+---------------------|
| UCB1           | _/Known/_ $0$      | _/Known/_ $1$   |                     |
| UCB1-Normal    | $-\infty$          | $\infty$        |                     |
| *UCB1-Uniform* | */Unknown/* $\ell$ | */Unknown/* $u$ | _/Fixed, Flat/_     |
| *UCB1-Power*   | _/Known/_ $0$      | */Unknown/* $u$ | */Rare near goals/* |


$\text{UCB1-Uniform}_i = \frac{u_i+l_i}{2} -(u_i-l_i)\sqrt{6n_i \log N}$

$\text{UCB1-Power}_i = \frac{u_ia_i}{a_i+1} -u_i\sqrt{6n_i \log N}$


* Results (SoCS 2024 Extended Abstract)

[[spng:extreme-results]]

* ECAI 2024 + SoCS 2024: Conclusion

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
+ *UCT is slow in*

  Classical Planning

  $\mu - c \sqrt{ \frac{\log N}{n_i} }$

+ *Why?*

  $h(s) \not\in [0,c]$

  (fixed range)
  
#+end_span4
#+begin_span4
+ (ECAI 2024)

  *Use Gaussian Bandit!*

  $\mu - \sigma \sqrt{ \log N }$

  ã€€
  
  $h(s) \in [-\infty,\infty]$

  ã€€

+ */STILL WRONG!/*

  $0 \le h(s)$
#+end_span4
#+begin_span4
+ (SoCS 2024)

  *Use Uniform / Power!*

  $\frac{u_i+l_i}{2} -(u_i-l_i)\sqrt{6n_i \log N}$

  $\frac{u_ia_i}{a_i+1} -u_i\sqrt{6n_i \log N}$

#+end_span4
#+end_row-fluid
#+end_container-fluid

#+begin_xlarge
+ Use bandits correctly!
#+end_xlarge

#+end_center



* Conclusion & Take-home message

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
+ è¨˜å·æ¥åœ°:
  + PDDLã‚’ *æ­£ã—ã* ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã‚ˆã†!
    + â†’ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«
+ ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹å­¦ç¿’:
  + RL : Reward Shaping ã‚’ *æ­£ã—ã* ã‚„ã‚ã†!
    + */Discouning/* , _/Undiscounting/_
  + SL : $h^*(s)$ ã‚’ *æ­£ã—ã* ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã‚ˆã†!
    + Truncated Gaussian
+ æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹å–„ / Bandit:
  + $h(s)$ ã‚’ *æ­£ã—ã* ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã‚ˆã†!
    + â†’ Gaussian, Power
#+end_span8
#+begin_span4
[[png:nesyarch/0b]]

#+begin_larger
#+begin_center
+ *æ­£ã—ã•ã¸ã®*

  *ã“ã ã‚ã‚Šã¯*

  *ã„ã¤ã‹*

  *å®Ÿã‚’çµã¶ã¯ãš*

  (å¤§å¤‰ã ã‘ã©)

#+end_center
#+end_larger
  
#+end_span4
#+end_row-fluid
#+end_container-fluid

# + æ„å‘³ã‚ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ã‚’ã‚„ã‚‹ã«ã¯
# 
#   PhD2å›åˆ†å‹‰å¼·ã™ã‚‹å¿…è¦ã‚ã‚Š (è¨˜å·ç³»+å­¦ç¿’ç³»)
# 
# + è¨˜å·ç³»ã®äººã¯ã€å­¦ç¿’ç³»ã®äººã‚’ãƒã‚«ã«ã—ã™ããªã„ã‚ˆã†ã«
# 
# + å­¦ç¿’ç³»ã®äººã¯ã€è¨˜å·ç³»ã®äºº/ã‚¿ã‚¹ã‚¯ã‚’è¦‹ãã³ã‚‰ãªã„ã‚ˆã†ã«

* Final Words

#+begin_container-fluid
#+begin_row-fluid
#+begin_span5
[[png:nesyarch/0b]]

+ Deep RL?
  + Repr. Learning
  + Heuristic Learning
  + Search Algorithm
  + Active Learning
  + Contiual Learning

#+end_span5
#+begin_span7
+ *Deep RL /sucks/*
+ *Deep RL is /Pseudoscience/*
  + Lack of reproducibility
    + Insufficient seeds
    + Leslie: 20 seeds minimum!
  + */1 change affects N components/*
+ _/The science/_ as a *sane* researcher:
  + _/Split/_ the architecture into parts
  + _/Study each part individually/_
  + _/Do correct mathematical modeling/_
#+end_span7
#+end_row-fluid
#+end_container-fluid

