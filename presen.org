#+TITLE:  ãƒ‹ãƒ¥ãƒ¼ãƒ­ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ Classical Planning
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: gif file:img/%s.gif
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+html_head: <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:500,900">
#+html_head_extra:
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center



#+begin_larger
Masataro Asai

(MIT-IBM Watson AI Lab, IBM Research, Cambridge)
#+end_larger



[[png:MIT-IBM]]

# [[spng:ibm-research]]

#+end_center

#+begin_center
è³ªå•ã¯Zoomã«æ›¸ãè¾¼ã¿
#+end_center

#+end_outline-text-1

* Part 1:

#+begin_xlarge
#+begin_center
è¨˜å·æ¥åœ°ã¨

ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«
#+end_center
#+end_xlarge

* ç«æ˜Ÿã«å–ã‚Šæ®‹ã•ã‚ŒãŸ! ã†ã‚ ã‚„ã£ã¹ ã©ã†ã—ã‚ˆã†?

[[png:mars/mars]]

* ã©ã£ã¡ã®ã€ŒçŸ¥æ€§ã€ãŒã„ã„ã§ã™ã‹?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3

[[png:mars/mars]]

#+end_span3
#+begin_span4
#+begin_center
*çµŒé¨“ã«åŸºã¥ãå‹˜ã ã‘* ã§ã©ã†ã«ã‹ãªã‚‹?

"è€ƒãˆã‚‹ãªã€æ„Ÿã˜ã‚" ã§ã†ã¾ãè¡Œãã¾ã™ã‹?

[[sgif:yoda]]

/Reflex (åå°„) agent/

#+end_center
#+end_span4
#+begin_span5
+
  #+begin_center
  ã¡ã‚‡ã£ã¨ *é ­ã‚’ä½¿ã£ã¦* è‡¨æ©Ÿå¿œå¤‰ã«å¯¾å¿œã—ã¾ã™ã‹?

  [[sgif:mars/yoda-hmph]]

  _/Deliberative (ç†Ÿè€ƒ) agent/_
  #+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_center
+ â†’ _ç§ãŒã‚„ã‚ŠãŸã„ã®ã¯ *ç†Ÿè€ƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ*_

  ã—ã‹ã‚‚ã€ _ç”Ÿã®ã‚»ãƒ³ã‚µãƒ¼å…¥åŠ›(ç”»åƒ)ã‚’ç›´æ¥ä½¿ãˆã‚‹ã‚„ã¤_
#+end_center

* å¤å…¸ãƒ—ãƒ©ãƒ³ãƒŠ (å¤å…¸è‡ªå‹•è¨ˆç”»ã‚½ãƒ«ãƒ): @@html:<br>@@ *8ãƒ‘ã‚ºãƒ«ã‚’ãƒŸãƒªç§’å˜ä½ã§è§£ã‘ã‚‹.*

è¨˜å·çš„æ¢ç´¢ãƒ»æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã¯çˆ†é€Ÿ

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span4
[[png:8puzzle-standard]]
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
#+end_span4
#+begin_span2
[[sgif:8puzzle]]
#+end_span2
#+end_row-fluid
#+end_container-fluid

/ã§ã‚‚ PDDL ãŒã‚ã‚‹ã¨ãã«ã—ã‹ é©ç”¨ä¸å¯èƒ½./

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
#+begin_src scheme
(:action       move-up ...
 :precondition (and      (empty ?x ?y-old) ...)
 :effects      (and (not (empty ?x ?y-old))...))
#+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

PDDL : Planning Domain Description Language

* å¼±ç‚¹: */ç”»åƒãƒ™ãƒ¼ã‚¹å…¥åŠ›/* ã«ãªã‚‹ã¨é©ç”¨ä¸å¯èƒ½

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+
   #+begin_center
   *ãªãœãªã‚‰*

   *ãƒ—ãƒ©ãƒ³ãƒŠã¯*

   *å®Ÿè¡Œã™ã‚‹ã®ã«*

   #+begin_larger
   */PDDL/*

   */ãƒ¢ãƒ‡ãƒ«ãŒ/*

   */å¿…è¦/!*
   #+end_larger
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

+ *Latplan (AAAI18, ICAPS19, IJCAI20, JAIR 22)* ã¯ã“ã®å•é¡Œã‚’è§£æ±º

  */ç”»åƒã«å¯¾ã™ã‚‹äº‹å‰çŸ¥è­˜ãªã—/* (ç”»åƒã«å¯¾ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©)

  ã€€ */äººã«ç”±æ¥ã™ã‚‹ãƒ©ãƒ™ãƒ«ãƒ»è¨˜å·ã¯ä¸€åˆ‡ãªã—/* : "ã‚¿ã‚¤ãƒ«ãŒ9ã¤ã‚ã‚‹", "å‹•ã"

* Latplan ã®ç›®æ¨™: */çŸ¥è­˜ç²å¾—ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è§£æ¶ˆ/*

# * We must *automate 2 processes*:

# * Knowledge-Acquisition Bottleneck:
#
# #+begin_quote
# The *cost of human* involved for converting *real-world problems* into the inputs for
# domain-independent *symbolic* systems
# #+end_quote



#+begin_container-fluid
#+begin_row-fluid
#+begin_span4

#+begin_alignright
*çŠ¶æ…‹é·ç§»ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿*
#+end_alignright

[[png:overview/1]]

#+end_span4
#+begin_span1






â†’


#+end_span1
#+begin_span7
#+begin_center
*PDDL Model*
#+end_center
#+begin_src scheme
(:action       move-up
 :precondition
 (and      (empty ?x ?y-old) ...)
 :effects
 (and (not (empty ?x ?y-old))...))
#+end_src

#+end_span7
#+end_row-fluid

#+begin_row-fluid
#+begin_span12
+ Latplan ãŒè‡ªå‹•åŒ–ã™ã¹ãä½œæ¥­ã¯ *2ã¤* ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯åˆ¥ã®ã‚¿ã‚¹ã‚¯:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. è¨˜å·ç”Ÿæˆ:*

  #+begin_center
  #+begin_larger
  */ã‚·ãƒ³ãƒœãƒ«/ = PDDLå†…ã§ä½¿ã‚ã‚Œã‚‹èªå½™*
  #+end_larger
  #+end_center

+
  #+begin_smaller
  | Types        | Examples                     |
  |--------------+------------------------------|
  | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | *panel_7*, *x_0*, *y_0* ...  |
  | è¿°èª | (*empty* ?x ?y)              |
  | å‘½é¡Œ | *p_28* = (empty x_0 y_0)     |
  | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ç²å¾—:*

  #+begin_center
  #+begin_larger
  */çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«ã‚’/*

  */ã‚·ãƒ³ãƒœãƒ«ã‚’ç”¨ã„ã¦/*

  */è¡¨ç¾ã™ã‚‹ã“ã¨/*
  #+end_larger
  #+end_center

+

  #+begin_center
  *When* /Empty(x, y_{old}) âˆ§ .../ ;

  *Then* /Â¬Empty(x,y_{old}) âˆ§/ ...
  #+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/2]]

ã€€

ã€€

ã€€

ã€€

** Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/1]]

ã€€

ã€€

ã€€

ã€€

** Latplan ã¯ PDDL ã‚’è‡ªå‹•ç”Ÿæˆ (IJCAI20, JAIR22)

[[png:15puzzle-with-pddl/0]]

#+begin_center
+ Latplan ãŒç”Ÿæˆãƒ»æ¥åœ°ã™ã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯ ã™ã¹ã¦ *ç„¡åã‚·ãƒ³ãƒœãƒ«* (=gensym=)

  *äººé–“ã®ä½¿ã† è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«* ã¯æ‰±ã‚ãªã„

+ è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«ã¸ã®æ¥åœ°ã¯ *å¯„ç”Ÿçš„* è¨˜å·æ¥åœ°(*) ã¨ã‚ˆã°ã‚Œã‚‹

  *ç†ç”±:* äººé–“ã®ä½œã£ãŸä¸–ç•Œã®é›¢æ•£åŒ–ã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚‰ (ä¾‹: *è™¹ğŸŒˆã¯7è‰²?*)

  #+begin_note
  (*) Taddeo, Mariarosaria, and Luciano Floridi. "Solving the symbol grounding problem: a critical review of fifteen years of research." Journal of Experimental & Theoretical Artificial Intelligence 17.4 (2005): 419-445.
  #+end_note
#+end_center


* Latplan ãŒè§£ã‘ã‚‹ãƒ‘ã‚ºãƒ«/ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å•é¡Œã®ä¾‹

[[spng:experiment/init-goal]]

8-puzzle, 15-puzzle, Lights-Out, Twisted Lights-out, *Blocksworld*, Sokoban

** æˆåŠŸç‡ (1ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚ãŸã‚Š40ã‚¿ã‚¹ã‚¯)

[[png:success]]

** æˆåŠŸä¾‹ / å¤±æ•—ä¾‹

[[spng:experiment/blocks-example]]

# [[spng:experiment/other-examples]]

* ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®è‚: *ç”Ÿæˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°* + @@html:<br>@@ _/ã¡ã‚ƒã‚“ã¨å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®ã“ã¨ã‚’å‹‰å¼·ã™ã‚‹ã“ã¨/_

+ Latplan ã¯ *PDDLã¨äº’æ›æ€§ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ç”»åƒã‹ã‚‰å­¦ç¿’ã™ã‚‹*.
+ *ã€ŒPDDLã¨äº’æ›æ€§ãŒã‚ã‚‹ã€ã£ã¦ã©ã†ã„ã†æ„å‘³ ?*

# *Background Terminology:*
#
# $z_{t} \rightarrow z_{t+1}$ is called a /progression/
# #+begin_alignright
# ($z_{t+1} \rightarrow z_{t}$ is called a /regression/)
# #+end_alignright

+ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: $a=\text{Eat}(\text{pizzağŸ•})$
+ å‰ææ¡ä»¶: $\text{pre}(a)=\lnot \text{full}() \land \text{have}(\text{pizzağŸ•})$ --- ç©ºè…¹ã‹ã¤ãƒ”ã‚¶ãŒã‚ã‚‹
+ åŠ¹æœ: *å‰Šé™¤åŠ¹æœ:* $\text{del}(a)=\{\text{have}(\text{pizzağŸ•}) \}$ --- ãƒ”ã‚¶ã¯ãªããªã£ã¦
+ åŠ¹æœ: *è¿½åŠ åŠ¹æœ:* $\text{add}(a)=\{\text{full}() \}$ --- æº€è…¹ã«ãªã‚‹

# + Action /eat(person, pizza)/
# + Precondition: Â¬full(person) âˆ§ have(person, pizza)
# + Delete effect: Â¬have(person, pizza)
# + Add effect: full(person)

# +
#   #+begin_src scheme
#   (:action eat :parameters (?person ?pizza)
#   #+end_src
# +
#   #+begin_src scheme
#   ã€€ :precondition (and (not (full ?person))
#   ã€€                   (have ?person ?pizza))
#   #+end_src
# +
#   #+begin_src scheme
#   ã€€ :effects (and (not (have ?person ?pizza)) ; delete effects
#   ã€€               (full ?person)))           ; add effects
#   #+end_src

+
  #+begin_quote
  _/çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«:/_ $z_{t+1} = (z_t \setminus \text{del}(a)\ )\ \cup\ \text{add}(a)$
  #+end_quote

  åŠ¹æœ $\text{del}(a), \text{add}(a)$ ã¯ *$z_t$ ã«ä¾å­˜ã—ãªã„*.

+ */ã‚ˆãã‚ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«/* ã¯ *ãã“ã‚’ã‚ã‹ã£ã¦ãªã„ï¼* (è‡ªå‹•è¨ˆç”»ã®äººãŒã„ãªã„)
  #+begin_quote
  */ã‚ˆãã‚ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«:/* $z_{t+1} = \text{ãªãã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ}(z_t, a)$
  #+end_quote

+

  #+begin_larger
  #+begin_center
  PDDLã«å¤‰æ›ã§ããªã„ â†’ */é«˜é€Ÿãªã‚½ãƒ«ãƒã‚’ä½¿ãˆãªã„/*
  #+end_center
  #+end_larger


# +
#   #+begin_quote
#   _/PDDL effect:/_ $z_{t+1} = (z_t \ \setminus \ \text{del}(a)\ )\ \cup\ \text{add}(a)$
#
#   */AAE's effect:/* $z_{t+1} = \text{apply}(z_t, a)$
#   #+end_quote
#
#   *In PDDL, effects should be consistent for each action*.
#
#   #+begin_alignright
#   *= Effects should be independent from $z_t$*
#   #+end_alignright

* é«˜é€Ÿã‚½ãƒ«ãƒã®ãŸã‚ã«PDDLè¨˜å·ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ @@html:<br>@@ â†’è¨€èªã®åˆ¶ç´„ã‚’ */NNãƒ¢ãƒ‡ãƒ«ã«åŸ‹ã‚è¾¼ã¾ãªã„ã¨ã„ã‘ãªã„/*

+ *PDDLã§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŠ¹æœã¨çŠ¶æ…‹ $z_t$ ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹.*

#+begin_center
+ â†’ $z_t$ ã¨ç‹¬ç«‹ãªå¤‰æ•° *e* ã‚’å°å…¥ã™ã‚‹å¿…è¦ã‚ã‚Šã€‚

+ ã€€
  
  [[png:space-ae/graphical-model-csae]]
#+end_center

# #+begin_smaller
# ç„¡åã‚·ãƒ³ãƒœãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€$z_t$ ã‚„ $a$ ã«ã¯ *é›¢æ•£éš ã‚Œå¤‰æ•°* ã‚’ä½¿ã† (Gumbel-Softmax)
# #+end_smaller

+ ã“ã‚Œã‚’ *é›¢æ•£çš„éš ã‚Œå¤‰æ•°* (Gumbel-Softmax) + *å¤‰åˆ†å­¦ç¿’* ã§è¨“ç·´ã€‚

  ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒæ•°å­¦çš„æ ¹æ‹ ã‚’ä¸ãˆã‚‹

* é›¢æ•£è¡¨ç¾å­¦ç¿’: Gumbel-Softmax VAE ã»ã‹

[[png:sae/state-ae-before]]

** é›¢æ•£è¡¨ç¾å­¦ç¿’: Gumbel-Softmax VAE ã»ã‹

[[png:sae/state-ae]]

+ 2017å¹´ãã‚‰ã„ä»¥é™ã€å¤šæ•°ã®æ‰‹æ³•ãŒè€ƒæ¡ˆã•ã‚Œã¦ã„ã‚‹

  (REBAR, RELAX, VQVAE, SQVAE, ...)

  +
    #+begin_alignright
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ã‚’ã™ã‚‹ãªã‚‰è¦ãƒã‚§ãƒƒã‚¯!
    #+end_alignright

* Important aspects of symbol grounding:                           :noexport:

Definition of symbol: *not* just state variables.

*Symbol* : A structure containing a pointer to a /name/ and a pointer to an /object/.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_src lisp
(defstruct symbol
  name
  object)
#+end_src
#+end_span4
#+begin_span4
#+begin_src python
class Symbol:
  name: string,
  obj: Any
#+end_src
#+end_span4
#+begin_span4
#+begin_src C
struct symbol {
    void* name;
    void* object;
}
#+end_src
#+end_span4
#+end_row-fluid
#+end_container-fluid

A symbol may point to *various things* through a hash table.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_src lisp
(symbol-value    'my-variable)
(symbol-function 'my-function)
(make-instance   'my-class)
#+end_src
#+end_span6
#+begin_span6
#+begin_src python
function_dictionary["myfunc"]
function_dictionary["myfunc"]
#+end_src
#+end_span6
#+end_row-fluid
#+end_container-fluid

* /ã‚·ãƒ³ãƒœãƒ«/ ã¯ä¸€ç¨®é¡ã ã‘ã§ã¯ãªã„

*/å‘½é¡Œã‚·ãƒ³ãƒœãƒ«/* (handempty, ...) vs. _/ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ³ãƒœãƒ«/_ (pickup,...).

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:space-ae/graphical-model-csae-only]]
#+end_span4
#+begin_span8
#+begin_center
+
  #+begin_larger
  ã“ã‚Œã‚‰ã¯ *åˆ¥ã®åå‰ç©ºé–“ã«å­˜åœ¨ã™ã‚‹*.
  #+end_larger

  ã€€

  è‡ªç„¶è¨€èªã‚·ãƒ³ãƒœãƒ«ã‚‚åŒæ§˜:

  ã€€ I */like/* an apple : å‹•è©

  ã€€ Thanks for the */like/* on Facebook! : åè©

  Common Lisp (Lisp-2) vs. Scheme (Lisp-1)
#+end_center

#+begin_larger
#+begin_alignright
+ â†’ *åˆ¥ã®åå‰ç©ºé–“* ã«ã‚ã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯

  *åˆ¥ã®è¨€èªåˆ¶ç´„ãŒå¿…è¦ã€‚*
#+end_alignright
#+end_larger
#+end_span8
#+end_row-fluid
#+end_container-fluid

* PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/1]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/2]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/3]]

ã€€

ã€€

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/4]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

ã€€

ã€€

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/5]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

*ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ³ãƒœãƒ«:* ã€Œåˆ¥ã®ã‚²ãƒ¼ãƒ ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€ â†’ åˆ¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

ã€€ãƒãƒªã‚ªã€ã‚¼ãƒ«ãƒ€ã€ãƒ‰ãƒ³ã‚­ãƒ¼ã‚³ãƒ³ã‚°ã€F-Zero ...

** PDDL: 6ã¤ã®åå‰ç©ºé–“, 6ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

[[png:namespace/6]]

*å•é¡Œã‚·ãƒ³ãƒœãƒ«:* ã€Œãƒãƒªã‚ªã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€â†’ åˆ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç”Ÿæˆ

ã€€Stage 1-1, Stage 1-2, ...

*ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ³ãƒœãƒ«:* ã€Œåˆ¥ã®ã‚²ãƒ¼ãƒ ã‚’è¡¨ã™ã‚·ãƒ³ãƒœãƒ«ã€ â†’ åˆ¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

ã€€ãƒãƒªã‚ªã€ã‚¼ãƒ«ãƒ€ã€ãƒ‰ãƒ³ã‚­ãƒ¼ã‚³ãƒ³ã‚°ã€F-Zero ...


* å®Ÿéš› Latplan ã‚’è¿°èªè«–ç†ã«æ‹¡å¼µå¯ (ICLR2021 workshop)

*Latplan/FOSAE++* : ç”»åƒå†…ã®ç•°ãªã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä½¿ãˆã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å­¦ç¿’

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
[[png:graphical-model-fosae++-nopatch2]]
#+end_span4
#+begin_span8
[[png:blocks-wide]]
#+end_span8
#+end_row-fluid
#+end_container-fluid


+ *è¿½åŠ åˆ¶ç´„:* Successor State Axiom (*éæŸç¸›å¤‰æ•°ã®ç¦æ­¢* [Reiter, 1991])

+ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: $a=\text{Eat}(\text{pizza}ğŸ•)$ : å¼•æ•° $\text{pizza}ğŸ•$
+
  #+begin_center
  *åŠ¹æœ:* */ç™ºè¡¨(ãƒ•ãƒ­ãƒ , ã‚¢ãƒ¼ãƒãƒ¼ãƒ‰ã‚³ã‚¢ã®æ–°ä½œ)/*
  #+end_center
+
  #+begin_alignright
  â†‘ $\text{pizza}ğŸ•$ ã‚’ä½¿ã‚ãªã„ã®ã§ç¦æ­¢
  #+end_alignright

+
  #+begin_larger
  #+begin_center
  æ–°ãŸãªåˆ¶ç´„ãŒ *è¿°èªã‚·ãƒ³ãƒœãƒ«* ã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¯èƒ½ã«ã—ãŸ
  #+end_center
  #+end_larger


* çµè«–                                                             :noexport:

#+begin_xlarge
è¨˜å·æ¥åœ°ã—ãŸã„ãªã‚‰

#+begin_center
*å¯¾è±¡è¨€èªã‚’ /ã¡ã‚ƒã‚“ã¨/ ç†è§£ã—ã¦*
#+end_center

#+begin_alignright
*/æ­£ã—ã„/ ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚ã†!*
#+end_alignright
#+end_xlarge




* è¨˜å·çš„è¨€èªã«æ¥åœ°ã—ãŸã„ãªã‚‰ã€å¯¾è±¡è¨€èªã‚’ã¡ã‚ƒã‚“ã¨ç†è§£ã—ã¦ */æ­£ã—ã„/* ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚Šã¾ã—ã‚‡ã† :noexport:

ä¾‹1:
#+begin_center
ç”»åƒå…¥åŠ› â†’ PDDL â†’ é«˜é€ŸPDDLã‚½ãƒ«ãƒ
#+end_center

ä¾‹2 (æœªç™ºè¡¨):
#+begin_center
+ éç·šå½¢çŠ¶æ…‹é·ç§»ãƒ¢ãƒ‡ãƒ« â†’ ç·šå½¢éš ã‚Œåˆ¶å¾¡ãƒ¢ãƒ‡ãƒ« â†’ é«˜é€Ÿç·šå½¢åˆ¶å¾¡ã‚½ãƒ«ãƒ
#+end_center

[[png:space-ae/graphical-model-style]]

#+begin_center
+ *Caution: It requires a /deep/ understanding of your formalism*

  *(e.g., PDDL, linear control).* Know what you are modeling!
#+end_center

* Extending the generative model (unpublished)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
#+begin_center
+ Lifted actions,

  Parameters *x*

  [[png:space-ae/graphical-model-fosae]]

  Predicates, too.

  *Compatible with First Order Logic.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Temporal actions,

  Action durations *d*

  [[png:space-ae/graphical-model-time]]

  *Compatible with LTL.*

#+end_center
#+end_span4
#+begin_span4
#+begin_center
+ Domain symbol *D*,

  Problem symbol *P*

  [[png:space-ae/graphical-model-domain]]

  Allows mixed-domain training?
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

# + Discrete-Continuous
#
#  [[png:space-ae/graphical-model-style]]



* Part 1: Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ *Goal: ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ*

  [[sgif:mars/yoda-hmph]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ *Latplan* ã§è¨˜å·æ¥åœ°

  [[sjpg:puzzle]]

#+end_center
#+end_span5
#+end_row-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ ç•°ãªã‚‹ã‚·ãƒ³ãƒœãƒ«ã¯  *åˆ¥ã€…ã®åˆ¶ç´„ãŒå¿…è¦*

  [[png:namespace/6]]
#+end_center
#+end_span7
#+begin_span5
#+begin_center
+ ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦

  *åˆ¶ç´„ã‚’ç²¾ç·»ã«è¨˜è¿°å¯èƒ½*

  [[png:space-ae/graphical-model-csae]]
#+end_center
#+end_span5
#+end_row-fluid
#+end_container-fluid

# #+begin_center
# Masataro Asai, Hiroshi Kajino, Alex Fukunaga, Christian Muise:
#
# *Classical Planning in Deep Latent Space.* Arxiv 2107.00110 -> JAIR
# #+end_center

+ è¨˜å·æ¥åœ°ã™ã‚‹ãªã‚‰ *å¯¾è±¡è¨€èªã‚’ /ã¡ã‚ƒã‚“ã¨/ ç†è§£ã—ã¦ /æ­£ã—ã/ ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã‚ˆã†!*


#+begin_center
è³ªå•: 2ã¤ã¾ã§
#+end_center

* Why _/action symbol grounding/_ is necessary?                    :noexport:

"designers always know the action space"? */Wrong!/*

Humans have a good understanding of low-level actuations...

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+
  #+begin_center
  Running
  #+end_center
  [[sjpg:triathlon/running]]
# sjpg:triathlon/bicycle
# sjpg:triathlon/swimming
#+end_span6
#+begin_span6
+
  #+begin_center
  Career Path
  #+end_center
  [[sjpg:triathlon/career]]

  #+begin_center
  Do you fully understand all of your opportunities?
  #+end_center
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
+ *Low-level Control*
#+end_center
#+end_span6
#+begin_span6
#+begin_center
+ */High-level Action/*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid



* Part 2:

#+begin_xlarge
ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®
#+begin_center
*/ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£/*
#+end_center
#+begin_alignright
ã‚’è€ƒãˆã‚‹
#+end_alignright
#+end_xlarge

#+begin_alignright
+ â†’ *è¡¨ç¾å­¦ç¿’ä»¥å¤–* ã® ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°+æ©Ÿæ¢°å­¦ç¿’
#+end_alignright


* ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/6]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/5]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/4]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/3]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/2]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/1]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

** ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0a]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* æ¢ç´¢ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã®å­¦ç¿’:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_xlarge
#+begin_center
RL

(ICAPS 2022)
#+end_center
#+end_xlarge
#+end_span6
#+begin_span6
#+begin_xlarge
#+begin_center
SL

(IJCAI 2024)
#+end_center
#+end_xlarge
#+end_span6
#+end_row-fluid
#+end_container-fluid

* ICAPS22

[[png:pddlrl/title]]

* Value function learning

SGD Loss function:

\[
\frac{1}{|B|}\sum_{s\in B} \frac{1}{2}\left| V(s) - \mathbb{E}_{a\in A} [Q(a, s)] \right|^2
\]

$B$: batches in the experience replay

#+begin_larger
Research Questions:
#+begin_center
+ *Q: RLã ã‘ã§ å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’è§£ã‘ã‚‹ã‹?* (A: *ä¸å¯*)

+ *Q: RLã ã‘ã§ ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã‹?* (A: *ä¸å¯*)

+ *Q: ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®é“å…·ã¨é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚Œã°?* */(A: å¯)/*
#+end_center
#+begin_alignright
+ Lessons learned: *å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ç”˜ãè¦‹ã‚‹ãª*
#+end_alignright
#+end_larger


* Why Classical Plannning is difficult for RL?

Rewards in Classical Planning:

#+begin_center
#+begin_larger
+ $r(s,a,s') = \left\{\begin{array}{ll} 1 & \text{if}\ s' \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  *This is extremely sparse.*

#+end_larger
#+end_center

+ */Any non-goal states/* are *equally /worthless/*

  + *No guidance* for RL until a goal,

  + which is *difficult to find* in the first place!

* Potential-Based */Reward Shaping/* (PBRS) @@html:<br>@@ for Sparse Rewards

Given a *potential function* $\phi(s)$, */redefine rewards/* as:

\begin{align}
\hat{r}(s, a, s') &= r(s, a, s') + \gamma \phi(s') - \phi(s). \label{eq:shaping1}
\end{align}

+ */Important:/* PBRS *preserves* the optimal value function

  \begin{align}
  V^*_\gamma(s) &= \hat{V}^*_\gamma(s) + \phi(s). \label{eq:shaping2}
  \end{align}

  #+begin_alignright
  #+begin_larger
  = *learning the residual from $\phi(s)$*
  #+end_larger
  #+end_alignright

#+begin_note
Ng, Harada, Russell. "Policy invariance under reward transformations: Theory and application to reward shaping." ICML. Vol. 99. 1999.
#+end_note

* Heuristic Function $h(s)$ in classical planning

+ *Distance estimate*
  + $h(s)=5 \quad \longleftrightarrow \quad s - s_1 - s_2 - s_3 - s_4 - \text{goal}$ (ãŠãŠã‚ˆã) 
  + Many implementations: $h^{\text{add}}$, $h^{\text{FF}}$, $h^{\text{CEA}}$, ...

    #+begin_smaller
    (Bonet&Geffner 01, Hoffmann 01, Helmert 08) ã‚‚ã£ã¨æ–°ã—ã„ã®ã‚‚ã‚ã‚‹
    #+end_smaller

#+begin_xlarge
+ *Heuristics satisfies the PBRS requirements*

+ 
  #+begin_alignright
  */Let's use $h(s)$ for PBRS!/*
  #+end_alignright
#+end_xlarge

* Issue 1: */Discounting/*

#+begin_larger
*RL : /discounted rewards/* â†” *Planning :* _/undiscounted costs/_
#+end_larger

ã€€

Our solution:

+ *Training* : Convert $h(s)\rightarrow h_{\gamma}(s)$ : */discounting/* $h(s)$ for PBRS

+
  \begin{align}
  \phi(s) &= h_\gamma(s) = \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.
  \end{align}

  # V_\gamma(s) &= \hat{V}_\gamma(s) + \phi(s) \quad \text{where}\\
  # \phi(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot (-1) = -\frac{1-\gamma^{h(s)}}{1-\gamma}
  # h(s) &= \sum_{t=1}^{h(s)} 1, & h_\gamma(s) &= \sum_{t=1}^{h(s)} \gamma^t\cdot 1 = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}.


+ *Planning* : Convert $V_{\gamma}(s)\rightarrow V(s)$ : _/undiscounting/_ a learned $V_{\gamma}(s)$

+   
  \begin{align}
  V_\gamma(s) = \dfrac{1 - \gamma^{V(s)}}{1 - \gamma}
  &\Leftrightarrow
  V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s)).
  \end{align}

* Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm6]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm5]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm4]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm3]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm2]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm1]]

** Issue 2: Handling */First-Order Logic Input/*

Our solution: *Neural Logic Machine (NLM)* [Dong et. al. ICLR 2019]

[[png:pddlrl/nlm]]

+ *Size* & *Permutation invariant* to FOL binary inputs

+ *Quite useful for various FOL tasks*
  + ILP [Dong et. al. ICLR 2019]
  + Regression (our work) 

* Results

Skipped

* ICAPS 2022 : Conclusion

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *Sparse Rewards*

  #+begin_smaller
  $r(s) = \left\{\begin{array}{ll} 1 & \text{if}\ s \ \text{is a goal}, \\ 0 & \text{otherwise}. \end{array}\right.$

  ã‚´ãƒ¼ãƒ«ã«ãŸã©ã‚Šç€ãã®ãŒ

  ã¨ã¦ã‚‚é›£ã—ã„
  #+end_smaller
#+end_span6
#+begin_span6
+ *Potential-Based*

  *Reward Shaping*

  $V^*_\gamma(s) = \hat{V}^*_\gamma(s) + \phi(s)$

  #+begin_smaller
  $\phi(s)=$ è·é›¢ã®è¦‹ç©ã‚‚ã‚Š $h(s)$

  Sparse Reward ã‚’è§£æ±º
  #+end_smaller
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Discounting*

  $h_\gamma(s)  = \dfrac{1 - \gamma^{h(s)}}{1 - \gamma}$

  $V(s) = \log_\gamma (1 - (1 - \gamma)V_\gamma(s))$

  #+begin_smaller
  ã“ã‚ŒãŒãªã„ã¨

  $h(s)=\infty$ ã®ã¨ãæ­»ã¬
  #+end_smaller
#+end_span6
#+begin_span6
+ *First-Order Logic Input*

  [[png:pddlrl/nlm]]

  #+begin_smaller
  ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®å•é¡Œã«æ±åŒ–
  #+end_smaller

#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_smaller
+ *Q: RLã ã‘ã§ å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’è§£ã‘ã‚‹ã‹?* (A: *ä¸å¯*)

  *Q: RLã ã‘ã§ ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã‹?* (A: *ä¸å¯*)

  *Q: ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®é“å…·ã¨é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚Œã°?* */(A: å¯)/*

  Lessons learned: */å¤å…¸ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ç”˜ãè¦‹ã‚‹ãª/*
#+end_smaller
#+end_center


* IJCAI24

[[png:truncated-gaussian/title]]



#+begin_xlarge
#+begin_center
RL å«Œã„ãªã‚“ã§ ... æ•™å¸«ã‚ã‚Šå­¦ç¿’!
#+end_center
#+end_xlarge

* Task: Learn the shortest path cost

[[png:truncated-gaussian/1b]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1a]]

** Task: Learn the shortest path cost

[[png:truncated-gaussian/1]]

#+begin_larger
#+begin_center
+ *Q: Can we exploit the /admissibility/ of heuristics* in training?
#+end_center
#+end_larger

* Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2j]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2i]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2h]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2g]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2f]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2e]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2d]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2c]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2b]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2a]]

** Stop using (Mean) Square Errors!

[[png:truncated-gaussian/2]]

* 

[[png:truncated-gaussian/3c]]

** 

[[png:truncated-gaussian/3b]]

** 

[[png:truncated-gaussian/3a]]

** 

[[png:truncated-gaussian/3]]

* Thinking with Distributions                                      :noexport:

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

+ We know $h^*\ge 0$
+ We know $h^*\ge h$ : *admissible heuristics*
+ *Why the heck* do we assign */non-zero probability/* to $h^*< 0$?
+ i.e. $\mathcal{N}(\mu,\sigma)$ *ignores* our */expert knowledge/* on $h^*$
+ It is not the *correct* distribution

* Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4e]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4d]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4c]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4b]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4a]]

** Thinking with Distributions

Is $\mathcal{N}(\mu,\sigma)$ the correct distribution for $h^*$?

[[png:truncated-gaussian/4]]

* What is the correct distribution?

+ Follow the */Maximum Entropy Principle/* (Jaynes 1957):

  #+begin_quote
  + /Choose the distribution with the *largest entropy*/
  + /among those that satisfy the *expert knowledge / constraint*./
  #+end_quote

+
  #+begin_alignright
  #+begin_larger
  â†ª We know a lower bound $h\le h^*$ !

  *This is our expert knowledge!*
  #+end_larger
  #+end_alignright

* What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5c]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5b]]

** What is the max-ent distribution for our assumption?

[[png:truncated-gaussian/5a]]

** Results 

[[png:truncated-gaussian/6]]

* IJCAI 2024 : Conclusion

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ *We learn* $h^*$

  [[png:truncated-gaussian/1]]

#+end_span6
#+begin_span6
+ *Square error = Gaussian*

  [[png:truncated-gaussian/3]]
#+end_span6
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *Gaussian is a lie*

  [[png:truncated-gaussian/4]]
#+end_span6
#+begin_span6
+ *Max-ent: Truncated Gaussian*

  [[png:truncated-gaussian/5a]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

Take-home message: *Whenever you see a square error, doubt it!*

* ç†æ€§çš„ãªå®Ÿä¸–ç•Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#+begin_container-fluid
#+begin_row-fluid
#+begin_span3
[[sgif:mars/yoda-hmph]]
#+end_span3
#+begin_span9
[[png:nesyarch/0b]]
#+end_span9
#+end_row-fluid
#+end_container-fluid

* æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 


